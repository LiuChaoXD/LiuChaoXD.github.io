[{"content":"æœ€è¿‘æ¯”è¾ƒç«çš„é€šè¿‡å¦ä¸€ç§é‡‡æ ·æ–¹æ³•æé«˜LLMçš„Reasoning abilityæˆ–è€…å¹»è§‰ç°è±¡ï¼š Entropix\nåŸºæœ¬æ¦‚å¿µ ç°åœ¨å¤§å¤šæ•°æµè¡Œçš„LLMæ¶æ„æ˜¯åŸºäºTransformer architectureçš„ã€‚è¿™ç§æ¶æ„é€šå¸¸æ¥è®²åŒ…å«ä¸€ä¸‹å‡ ä¸ªå…³é”®éƒ¨åˆ†ï¼š\nEmbedding Layerï¼šç”¨æ¥å°†è¾“å…¥çš„tokenè½¬åŒ–ä¸ºvector Self-Attention Layersï¼šè‡ªæ³¨æ„åŠ›å±‚ï¼Œå°±æ˜¯ç½‘ç»œè‡ªåŠ¨å­¦ä¹ ç”¨æˆ·è¾“å…¥ä¸€æ®µæ–‡æœ¬ä¸­ï¼Œæ‰€æœ‰æ–‡æœ¬ä¹‹é—´çš„å…³ç³» Feed-Forward Layersï¼šç”¨æ¥è½¬åŒ–è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡º Layer Normalizationï¼šç”¨æ¥ç¨³å®šå­¦ä¹  %%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%% graph LR classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Input]) --\u003e G(Embedding):::blue G --\u003e B(Self-Attention):::pink B --\u003e C(Layer Norm):::orange C --\u003e D(Feed Forward):::red D --\u003e E(Layer Norm):::green E --\u003e F([Output]) LLMå¦‚ä½•generate textæˆ–è€…completion step 1: input processingï¼ˆè¾“å…¥å¤„ç†ï¼‰ï¼Œå³å°†input textå…ˆè¿›è¡Œtokenizationï¼Œç„¶åé€šè¿‡embeddingå°†å…¶æ˜ å°„åˆ°vectorç©ºé—´ä¸­ step 2ï¼šForward processingï¼ˆå‰å‘å¤„ç†ï¼‰ï¼Œå³ç»è¿‡embeddingåé€šè¿‡self-attentionï¼Œlayer normï¼Œfeed-forward ç­‰ï¼Œæœ€ç»ˆè·å¾—æ‰€æœ‰ä¸‹ä¸€ä¸ªå¯èƒ½tokençš„logits step 3: Samplingï¼ˆé‡‡æ ·ï¼‰ï¼Œè¿™é‡Œå°±æ˜¯è¯¥ç¯‡æ–‡ç« æŠ€æœ¯çš„å…³æ³¨ç‚¹ã€‚å›è¿‡å¤´æ¥ï¼Œç°åœ¨å¤§å¤šæ•°èƒ½å½±å“é‡‡æ ·ç»“æœçš„å‚æ•°æœ‰temperatureï¼ˆæ¸©åº¦ç³»æ•°ï¼‰ï¼Œtop-pï¼Œtop-k step 4: Repeat ï¼ˆé‡å¤ä¸Šè¿°æ­¥éª¤ï¼‰ï¼šå³å½“é‡‡æ ·å¥½äº†ä¸‹ä¸€ä¸ªtokenåï¼Œä¼šå°†è¯¥tokenæ·»åŠ åˆ°input textçš„æœ«å°¾ï¼Œå³æ­¤æ—¶çš„è¾“å…¥å˜ä¸ºäº†input text + â€œnext tokenâ€ï¼Œç„¶åé‡‡æ ·â€œnext next tokenâ€ Logitsçš„ä½œç”¨ logitså°±æ˜¯æ¦‚ç‡ï¼Œä¸»è¦æ˜¯é€šè¿‡softmaxå‡½æ•°å»å°†æœ€åä¸€å±‚çš„è¾“å‡ºè½¬åŒ–ä¸ºæ€»å’Œä¸º1çš„æ¦‚ç‡ã€‚å³\n$ P(token_i) = \\frac{e^{logit_i}}{\\sum_j e^{logit_j}} $\nSelf-Attentionçš„ä½œç”¨ self-attentionå°±æ˜¯å¯ä»¥è®©LLMå»å…³æ³¨åˆ°ä¸€æ®µæ–‡æœ¬ä¸­ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„å…³ç³»ã€‚æ¯”å¦‚â€œæˆ‘ä»Šå¤©è¯»äº†ä¸‰å››ç¯‡è®ºæ–‡ï¼Œä»–ä»¬éƒ½æ˜¯å…³äºLLM æ¨ç†èƒ½åŠ›çš„å­¦æœ¯è®ºæ–‡â€ï¼Œé‚£ä¹ˆè¿™æ®µæ–‡æœ¬ä¸­â€œä¸‰å››ç¯‡è®ºæ–‡â€å’Œâ€œæ¨ç†èƒ½åŠ›â€ä»¥åŠâ€œå­¦æœ¯è®ºæ–‡â€ï¼Œä»–ä»¬ä¹‹é—´çš„å…³ç³»å°±å¾ˆå¯†åˆ‡ã€‚self-attentionæœºåˆ¶å¯ä»¥è®©LLMå»å…³æ³¨åˆ°è¿™äº›å…ƒç´ ä¹‹é—´çš„å…³ç³»ã€‚\nğŸ’¡ è¿™é‡Œå¯ä»¥æå‡ºä¸€ä¸ªå‡è®¾é—®é¢˜ï¼šåŠ å…¥attentionå†…éƒ¨ä¹‹é—´æ•æ‰åˆ°çš„å…³ç³»ï¼Œå¾ˆå¤æ‚ï¼ˆæ¯”å¦‚attention weightsåˆ†å¸ƒå¾ˆæ•£ï¼‰ï¼Œé‚£æ˜¯ä¸æ˜¯å¯ä»¥ç†è§£ä¸ºç”¨æˆ·è¾“å…¥çš„è¿™æ®µæ–‡æœ¬æ¯”è¾ƒå¤æ‚ï¼ˆæˆ–è€…ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ä»»åŠ¡å¤æ‚ã€å¾ˆéš¾ï¼‰ è¿™ä¸€ä¸ªå‡è®¾ï¼Œå°±å’Œæ–‡ç« çš„æ–¹æ³•æœ‰å…³ï½\nè¯­è¨€æ¨¡å‹ä¸­çš„Entropyï¼ˆç†µï¼‰ ç†µï¼ˆEntropyï¼‰ ç†µçš„å®šä¹‰ä¸º\n$$ H = -\\sum_{i} p_i \\log_2(p_i) $$\n$p_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªtokençš„æ¦‚ç‡ã€‚æ­¤æ—¶å‡è®¾ä¸€ç§æƒ…å†µï¼šç”¨æˆ·è¾“å…¥â€œæˆ‘è¦å»è¯»è®ºâ€ï¼Œ å¦‚æœvocabulary sizeä¸º32000\nå¦‚æœï¼šæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ˜¯â€œæ–‡â€çš„æ¦‚ç‡ä¸º1ï¼Œå…¶ä»–31999ä¸ªtokenæ¦‚ç‡éƒ½æ˜¯0ï¼Œåˆ™ç†µä¸º0 å¦‚æœï¼šæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokenæ˜¯â€œæ–‡â€çš„æ¦‚ç‡ä¸º0.1ï¼Œå…¶ä»–31999ä¸ªtokenï¼Œåˆ†åˆ«ä¸º $[0.01, 0.2, 0.0043, \\cdots, 0.21]$ï¼Œé‚£ä¹ˆæŒ‰ç…§ç†µçš„å®šä¹‰è®¡ç®—ä¸‹æ¥ï¼Œæ­¤æ—¶ç†µä¼šæ¯”è¾ƒå¤§ã€‚ ğŸ’¡ ç›´è§‚ç†è§£ï¼š\nå¦‚æœç†µå°ï¼Œè¡¨ç¤ºé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆæŸä¸ªtokenï¼‰çš„æ¦‚ç‡å¾ˆé«˜ï¼Œå…¶ä»–tokençš„æ¦‚ç‡å¾ˆå°ã€‚æ¨¡å‹å¯¹é¢„æµ‹çš„ç»“æœæ¯”è¾ƒç¬ƒå®šã€‚ å¦‚æœç†µå¤§ï¼Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆæŸä¸€äº›tokensï¼‰çš„æ¦‚ç‡éƒ½å¾ˆé«˜ï¼Œå…¶ä»–tokenæ¦‚ç‡æ¯”è¾ƒå°ã€‚é‚£ä¹ˆæ¨¡å‹å¯¹é¢„æµ‹çš„ç»“æœå°±ä¸ç¬ƒå®šã€‚ ç†µçš„æ–¹å·®ï¼ˆVarentropyï¼‰ ç›´è§‚ç†è§£å°±æ˜¯åœ¨ä¸€ä¸ªä½ç½®ä¸Šã€é¢„æµ‹çš„tokençš„ç†µå˜åŒ–æœ‰å¤šå¤§ã€‚\nå…·ä½“è®¡ç®—æ–¹æ³•ä¸ºï¼šå¯¹äºå½“å‰ä½ç½®â€œæˆ‘è¦å»è¯»è®º+å½“å‰ä½ç½®â€\nè®¡ç®—è¯¥ä½ç½®çš„tokençš„æ¦‚ç‡probabilityï¼ˆç»è¿‡softmaxè·å¾—çš„logitsï¼‰ï¼Œä»¥åŠlog probability è®¡ç®—ç†µ è®¡ç®—negative log probabilityå’Œç†µentropyä¹‹é—´çš„å·®å€¼çš„å¹³æ–¹ ğŸ’¡ ç†µçš„æ–¹å·®ï¼šå¯ä»¥ç›´è§‚ç†è§£ä¸ºæ¨¡å‹å¯¹é¢„æµ‹å½“å‰ä½ç½®tokençš„ä¸ç¡®å®šæ€§æœ‰å¤šé«˜ã€‚æ–¹å·®è¶Šå¤§ï¼Œåˆ™æ¨¡å‹é¢„æµ‹è¶Šä¸ç¡®å®š\nç†µå’Œç†µçš„æ–¹å·®å…·ä½“è®¡ç®—ä»£ç å¦‚ä¸‹ï¼ˆæ¥è‡ªå®˜æ–¹çš„ä»£ç åº“ï¼‰ï¼š\n1 2 3 4 5 6 7 8 9 10 LN_2 = 0.69314718056 # ln(2) = 1.0 / LOG2_E @jax.jit def calculate_varentropy_logsoftmax(logits: jnp.ndarray, axis: int = -1) -\u0026gt; Tuple[jnp.ndarray, jnp.ndarray]: \u0026#34;\u0026#34;\u0026#34;Calculate the entropy and varentropy of the probability distribution using logsoftmax.\u0026#34;\u0026#34;\u0026#34; log_probs = jax.nn.log_softmax(logits, axis=axis) probs = jnp.exp(log_probs) entropy = -jnp.sum(probs * log_probs, axis=axis) / LN_2 # Convert to base-2 varentropy = jnp.sum(probs * (log_probs / LN_2 + entropy[..., None])**2, axis=axis) return entropy, varentropy æ€»ç»“å‡ ç§æƒ…å†µï¼š\nLow Entropy, Low Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰å¾ˆé«˜çš„confidenceå’Œconsistency. è¿™ç§æ¨¡å¼ä¸‹ï¼Œå¯èƒ½è´ªå©ªé‡‡æ ·å°±æ¯”è¾ƒé€‚åˆgreedy sampling High Entropy, Low Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰ä¸€è‡´çš„ä¸ç¡®å®šã€‚è¿™ç§æ¨¡å¼ä¸‹ï¼Œå¯èƒ½clarification insertionæˆ–è€…increased explorationæ¯”è¾ƒåˆé€‚ Low Entropy, High Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰å¤šç§ä¸åŒçš„confidenceã€‚è¿™ç§æ¨¡å¼ä¸‹æ¢ç´¢ï¼ˆexplorationï¼‰ samplingæ¯”è¾ƒåˆé€‚. High Entropy, High Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œä¸ç¡®å®šä¹Ÿä¸ä¸€è‡´ã€‚è¿™ç§æ¨¡å¼ä¸‹æ¨¡å‹å…·æœ‰é«˜åº¦çš„ä¸ä¸€è‡´ä¸ç¡®å®šæ€§ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦è°ƒæ•´ä¸€äº›ä¾‹å¦‚top-pï¼Œtemperatureï¼Œtop-kå‚æ•° è¯­è¨€æ¨¡å‹ä¸­çš„Attention Self-Attention å†…éƒ¨ %%{init: {'theme':'base'}}%% graph TD classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Input Tokens]) --\u003e B[Multi-Head Attention]:::blue B --\u003e C[Attention Head 1]:::pink B --\u003e D[Attention Head 2]:::pink B --\u003e E[...]:::pink B --\u003e F[Attention Head N]:::pink C --\u003e G[Concatenate \u0026 Linear Transform]:::orange D --\u003e G E --\u003e G F --\u003e G G --\u003e H([Output]) self-attentionä¸­æœ‰å¤šå¤´æœºåˆ¶ï¼Œé‚£ä¹ˆè¿™é‡Œå°±è¦å‡è®¾ä¸¤ç§æƒ…å†µï¼š\nåœ¨ä¸€ä¸ªattentionå¤´ä¸­ï¼š å¦‚æœattention weightsçš„ç†µentropyæ¯”è¾ƒå¤§ï¼Œé‚£ä¹ˆè¡¨ç¤ºæ¨¡å‹å…³æ³¨åˆ°äº†è®¸å¤šä¸åŒçš„tokens å¦‚æœattention weightsçš„ç†µentropyæ¯”è¾ƒå°ï¼Œé‚£ä¹ˆè¡¨ç¤ºæ¨¡å‹åªå…³æ³¨åˆ°äº†å‡ ä¸ªç‰¹åˆ«çš„tokens åœ¨å¤šä¸ªattentionå¤´ä¸­ å¦‚æœå¤šä¸ªå¤´çš„attention weightséå¸¸ç›¸è¿‘ï¼Œé‚£ä¹ˆè¡¨ç¤ºæ¨¡å‹æ¨¡å‹çš„å¤šä¸ªå¤´ï¼ŒåŒæ—¶éƒ½å…³æ³¨åˆ°äº†å‡ ä¸ªç‰¹åˆ«çš„token å¦‚æœå¤šä¸ªå¤´çš„attention weightså·®å¼‚éå¸¸å¤§ï¼Œé‚£ä¹ˆè¡¨ç¤ºæ¨¡å‹çš„å¤šä¸ªå¤´ï¼Œå…³æ³¨çš„æ˜¯ä¸åŒçš„tokens è¿™é‡Œå°±å¯å¼•å‡ºä¸¤ä¸ªæ¦‚å¿µï¼šAttention Entropy å’ŒAttention Agreement\nAttention Entropyï¼š\n1 2 attention_probs = jax.nn.softmax(attention_scores, axis=-1) attn_entropy = -jnp.sum(attention_probs * jnp.log2(jnp.clip(attention_probs, 1e-10, 1.0)), axis=-1) Attention Agreement\n1 2 mean_attention = jnp.mean(attention_probs, axis=1) agreement = jnp.mean(jnp.abs(attention_probs - mean_attention[:, None, :]), axis=(1, 2)) ğŸ’¡ é«˜attention entropyï¼Œä¼šå¢åŠ æ¢ç´¢ï¼ˆexplorationï¼‰åœ¨é‡‡æ ·ä¸­çš„ä½œç”¨ ä½attention agreementï¼Œä¼šéœ€è¦è°ƒæ•´top-pï¼Œtemperatureï¼Œtop-kå‚æ•° ä¸åŒå±‚Self-Attentionï¼šInteraction Strength Interaction Strengthçš„å®šä¹‰ä¸ºï¼šæ‰€æœ‰å±‚çš„attention scoreçš„ç»å¯¹å€¼çš„å’Œï¼ˆæ³¨æ„æ˜¯æ‰€æœ‰ï¼‰\n$$ \\text{Interaction Strength} = \\frac{1}{L \\cdot H \\cdot N} \\sum_{l=1}^L \\sum_{h=1}^H \\sum_{i=1}^N \\sum_{j=1}^N |A_{l,h,i,j}| $$\n$L$ ï¼šå±‚æ•° $H$ ï¼š attention headsçš„ä¸ªæ•° $N$ ï¼š è¾“å…¥æ–‡æœ¬è½¬åŒ–ä¸ºtokenä¹‹åçš„é•¿åº¦ $A_{l,h,i,j}$ ï¼š è¡¨ç¤ºç¬¬ $l$ å±‚ï¼Œ ç¬¬ $h$ ä¸ªattention headçš„ï¼Œç¬¬ $i$ ä¸ªä½ç½® å’Œ ç¬¬ $j$ ä¸ªä½ç½®çš„attention score ğŸ’¡ ç›´è§‚çš„æ„Ÿå—æ˜¯ï¼šå¦‚æœè¿™ä¸ªinteraction strengthè¶Šé«˜ï¼Œåˆ™è¡¨ç¤ºæ–‡æœ¬ä¹‹é—´çš„å…³ç³»è¶Šå¼ºçƒˆã€‚æ­¤æ—¶å°±éœ€è¦å¯¹samplingç­–ç•¥åšä¸€å®šçš„è°ƒæ•´ï¼Ÿ\ninteraction strengthçš„è®¡ç®—æ­¥éª¤ï¼š\nStep 1ï¼šæå–æ‰€æœ‰çš„attention scoreï¼Œæ³¨æ„æ˜¯æ‰€æœ‰\nStep 2ï¼š æ‰€æœ‰çš„attention scoreç”¨ç»å¯¹å€¼\nStep 3ï¼šè®¡ç®—å‡å€¼\nä»£ç ï¼š\n1 interaction_strength = jnp.mean(jnp.abs(attention_scores), axis=(1, 2, 3)) Sampling ç­–ç•¥è°ƒæ•´ ä»‹ç»æœ€ç»ˆçš„é‡‡æ ·ç­–ç•¥ä¹‹å‰ï¼Œå…ˆå›é¡¾ä¸€ä¸‹å‰æ–‡ä¸­çš„ç›¸å…³å‚æ•°ï¼š\nlogits entropyï¼š é¢„æµ‹ä¸‹ä¸€ä¸ªtokensçš„logitsçš„ç†µ varentropy ï¼ˆvariance of logits entropyï¼‰ï¼šç†µçš„æ–¹å·® attention entropy ï¼š attention scoreçš„ç†µ attention agreementï¼šå¤šä¸ªattention headä¹‹é—´çš„attention ä¸€è‡´æ€§ interaction strengthï¼š æ‰€æœ‰å±‚çš„attention score ç²—ç•¥çš„è°ƒæ•´å¯ä»¥çœ‹å¦‚ä¸‹çš„å›¾\n%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%% graph LR classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff subgraph Metrics subgraph Uncertainty LU[Logits Uncertainty]:::blue AU[Attention Uncertainty]:::blue end A[Agreement]:::pink IS[Interaction Strength]:::orange end subgraph Sampling Parameters T[Temperature] TP[TOP-P] MP[MIN-P] TK[TOP-K] end LU --\u003e|Increate With| T LU --\u003e|Decrease With| T LU --\u003e|Increase With| TP AU --\u003e|Increase With| TP A --\u003e|Decrease With| T A --\u003e|Decrease With| TP A --\u003e|Decrease With| MP A --\u003e|Increase With| MP IS --\u003e|Increase With| TP IS --\u003e|Increase With| TK style Uncertainty fill:#e6e6e6,stroke:#666,stroke-width:1px %% Color coding for increases and decreases linkStyle 0,2,3,7,8,9 stroke:#FF0000,color:#FF0000 linkStyle 1,4,5,6 stroke:#0000FF,color:#0000FF,stroke-dasharray: 3 3 Temperature ç³»æ•°è°ƒæ•´ %%{init: {'theme':'base'}}%% graph TD classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Logits Uncertainty]):::blue --\u003e D[Temperature]:::green B([Attention Uncertainty]):::blue --\u003e D C([Agreement]):::blue --\u003e D D --\u003e E([Final Temperature]):::orange æ¸©åº¦ç³»æ•°çš„è°ƒæ•´ç­–ç•¥ï¼š $T = T_{base} * (1 + 0.3 * U_{logits} + 0.2 * U_{attn} - 0.2 * A)$\nå…¶ä¸­ $T_{base}$ å°±æ˜¯æœªç»è°ƒæ•´æˆ–è€…é»˜è®¤çš„Temperatureï¼Œ $U_{logits}$ å°±æ˜¯ entropy + varentropy $U_{attn}$ å°±æ˜¯ attention entropy + attention varentropy $A$ å°±æ˜¯ attention agreement TOP-På’ŒTOP-Kè°ƒæ•´ç­–ç•¥ TOP-K 1 top_k_adj = max(5, int(top_k * (1 + 0.3 * interaction_strength - 0.2 * agreement))) TOP-P 1 top_p_adj = jnp.clip(base_top_p * (1 + 0.1 * metrics[\u0026#34;attn_varentropy\u0026#34;]), 0.1, 1.0) Minimum Probability Threshold 1 min_p = jnp.clip(base_min_p * (1 - 0.5 * logits_uncertainty), 0.01, 0.5) Implementation entropixä¼šåœ¨æœ€ç»ˆé‡‡æ ·å‰è®¡ç®—å„ç§metricsç„¶ååšå‡ºé€‚åº”æ€§çš„è°ƒæ•´ï¼Œç”¨æ¥æ”¹å˜æˆ–è€…å¢å¼ºé‡‡æ ·çš„ç¡®å®šæ€§ã€‚å¦‚ä¸‹å›¾æ‰€ç¤º\n%%{init: {'theme':'base'}}%% graph TD classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Calculate Metrics]):::blue --\u003e B{Evaluate Entropy and Varentropy}:::pink B --\u003e|Low E, Low V| C[Greedy Sampling]:::orange B --\u003e|High E, Low V| D[Clarification Insertion]:::orange B --\u003e|Low E, High V| E[Exploration Sampling]:::orange B --\u003e|High E, High V| F[High Uncertainty Sampling]:::orange B --\u003e|Moderate Values| G[Adaptive Sampling]:::orange C --\u003e H([Generate Token]):::green D --\u003e H E --\u003e H F --\u003e H G --\u003e H æ³¨æ„è¿™é‡Œçš„E å’ŒVè¡¨ç¤ºçš„æ˜¯æ ¹æ®å¯¹next tokené¢„æµ‹çš„logitsï¼Œæ‰€è·å¾—çš„entropyå’Œvarentropyã€‚å³ä¸åŒç­–ç•¥çš„triggeræ˜¯é€šè¿‡logitsçš„ç†µå’Œç†µçš„æ–¹å·®å»åšçš„ã€‚ç„¶åtriggerä¸åŒçš„strategyï¼Œé‡‡ç”¨ä¸åŒçš„é‡‡æ ·è°ƒæ•´ç­–ç•¥ã€‚\nä¸ºäº†æ–¹ä¾¿ç†è§£å†æ¬¡å°†å‰æ–‡ç²˜è´´åˆ°è¿™é‡Œ\næ€»ç»“å‡ ç§æƒ…å†µï¼š\nLow Entropy, Low Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰å¾ˆé«˜çš„confidenceå’Œconsistency. è¿™ç§æ¨¡å¼ä¸‹ï¼Œå¯èƒ½è´ªå©ªé‡‡æ ·å°±æ¯”è¾ƒé€‚åˆgreedy sampling High Entropy, Low Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰ä¸€è‡´çš„ä¸ç¡®å®šã€‚è¿™ç§æ¨¡å¼ä¸‹ï¼Œå¯èƒ½clarification insertionæˆ–è€…increased explorationæ¯”è¾ƒåˆé€‚ Low Entropy, High Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰å¤šç§ä¸åŒçš„confidenceã€‚è¿™ç§æ¨¡å¼ä¸‹æ¢ç´¢ï¼ˆexplorationï¼‰ samplingæ¯”è¾ƒåˆé€‚. High Entropy, High Varentropy: æ¨¡å‹å¯¹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtokenï¼Œä¸ç¡®å®šä¹Ÿä¸ä¸€è‡´ã€‚è¿™ç§æ¨¡å¼ä¸‹æ¨¡å‹å…·æœ‰é«˜åº¦çš„ä¸ä¸€è‡´ä¸ç¡®å®šæ€§ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦è°ƒæ•´ä¸€äº›ä¾‹å¦‚top-pï¼Œtemperatureï¼Œtop-kå‚æ•° Metricsè®¡ç®— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def calculate_metrics(logits: jnp.ndarray, attention_scores: jnp.ndarray) -\u0026gt; Dict[str, jnp.ndarray]: entropy, varentropy = calculate_varentropy_logsoftmax(logits) // è®¡ç®—logitsçš„ç†µå’Œç†µçš„æ–¹å·® attention_probs = jax.nn.softmax(attention_scores, axis=-1) // è®¡ç®—é€šè¿‡attention scoresè·å¾—attention probabilityï¼Œä¸»è¦ç”¨æ¥è®¡ç®—attention entropyå’Œattention varentropy attn_entropy = -jnp.sum(attention_probs * jnp.log2(jnp.clip(attention_probs, 1e-10, 1.0)), axis=-1) attn_varentropy = jnp.var(attn_entropy, axis=-1) mean_attention = jnp.mean(attention_probs, axis=1) // è®¡ç®—æ‰€æœ‰attentionçš„å‡å€¼ agreement = jnp.mean(jnp.abs(attention_probs - mean_attention[:, None, :]), axis=(1, 2)) //è®¡ç®—attentionçš„agreement interaction_strength = jnp.mean(jnp.abs(attention_scores), axis=(1, 2, 3)) // è®¡ç®—interaction strength return { \u0026#34;logits_entropy\u0026#34;: jnp.mean(entropy), \u0026#34;logits_varentropy\u0026#34;: jnp.mean(varentropy), \u0026#34;attn_entropy\u0026#34;: jnp.mean(attn_entropy), \u0026#34;attn_varentropy\u0026#34;: jnp.mean(attn_varentropy), \u0026#34;agreement\u0026#34;: jnp.mean(agreement), \u0026#34;interaction_strength\u0026#34;: interaction_strength } Samplingä»£ç  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def sample(gen_tokens: jax.Array, logits: jax.Array, attention_scores: jax.Array, cfg: SamplerConfig, clarifying_question_token: int = 2564, key=jax.random.PRNGKey(1337)) -\u0026gt; jax.Array: metrics = calculate_metrics(logits, attention_scores) **ent**, **vent** = metrics[\u0026#34;logits_entropy\u0026#34;], metrics[\u0026#34;logits_varentropy\u0026#34;] attn_ent, attn_vent = metrics[\u0026#34;attn_entropy\u0026#34;], metrics[\u0026#34;attn_varentropy\u0026#34;] agreement = metrics[\u0026#34;agreement\u0026#34;] interaction_strength = metrics[\u0026#34;interaction_strength\u0026#34;] # Low Entropy, Low Varentropy: \u0026#34;flowing with unspoken intent\u0026#34; if **ent** \u0026lt; cfg.low_ent_thresh and **vent** \u0026lt; cfg.low_vent_thresh: //æ³¨æ„è¿™é‡Œæ˜¯é€šè¿‡ent å’Œ ventå»åˆ¤æ–­é‡‡ç”¨å“ªä¸€ç§adaptionç­–ç•¥ã€‚ return jnp.argmax(logits[:, -1], axis=-1, keepdims=True).astype(jnp.int32) # High Entropy, Low Varentropy: \u0026#34;treading carefully, asking clarifying questions\u0026#34; elif ent \u0026gt; cfg.high_ent_thresh and vent \u0026lt; cfg.low_vent_thresh: //è¿™é‡Œéœ€è¦æ’å…¥ä¸€ä¸ªclarificationé—®é¢˜ï¼Œé—®æ¸…æ¥šåˆ°åº•ç”¨æˆ·çš„é—®é¢˜ # Insert a clarifying question token if not already present if not jnp.isin(gen_tokens[:,-1], clarifying_question_token).any(): return jnp.array([[clarifying_question_token]]) else: # If we\u0026#39;ve just asked a question, sample with slightly higher temperature temp_adj = cfg.helv_attn_ent_offset + cfg.helv_attn_ent_coef * attn_ent # Increase temperature based on attention entropy return _sample(logits, temperature=min(1.5, cfg.temp * temp_adj), top_p=cfg.top_p, top_k=cfg.top_k, min_p=cfg.min_p, key=key) # Low Entropy, High Varentropy: \u0026#34;exploring forks in the path\u0026#34; elif ent \u0026lt; cfg.high_ent_thresh and vent \u0026gt; cfg.high_vent_thresh: temp_adj = cfg.lehv_interaction_strength_offset + cfg.lehv_interaction_strength_coef * interaction_strength # Increase temperature based on interaction strength top_k_adj = max(5, int(cfg.top_k * (1 + 0.5 * (1 - agreement)))) # Increase top_k when agreement is low return _sample(logits, temperature=min(1.5, cfg.temp * temp_adj), top_p=cfg.top_p, top_k=top_k_adj, min_p=cfg.min_p, key=key) # High Entropy, High Varentropy: \u0026#34;resampling in the mist\u0026#34; elif ent \u0026gt; cfg.med_ent_thresh and vent \u0026gt; cfg.high_vent_thresh: # Use high temperature and adjusted top_p based on attention metrics temp_adj = cfg.hehv_attn_vent_offset + cfg.hehv_attn_vent_coef * attn_vent # Increase temperature based on attention varentropy top_p_adj = max(0.5, cfg.top_p - cfg.hehv_attn_ent_coef * attn_ent) # Decrease top_p when attention entropy is high return _sample(logits, temperature=max(2.0, cfg.temp * temp_adj), top_p=top_p_adj, top_k=cfg.top_k, min_p=cfg.min_p, key=key) # Middle ground: use adaptive sampling else: logits_uncertainty = metrics[\u0026#34;logits_entropy\u0026#34;] + metrics[\u0026#34;logits_varentropy\u0026#34;] attn_uncertainty = metrics[\u0026#34;attn_entropy\u0026#34;] + metrics[\u0026#34;attn_varentropy\u0026#34;] temperature = cfg.temp * (1 + cfg.ada_temp_logits * logits_uncertainty + cfg.ada_temp_attn * attn_uncertainty - cfg.ada_temp_agree * metrics[\u0026#34;agreement\u0026#34;]) top_p = jnp.clip(cfg.top_p * (1 + cfg.ada_top_p * metrics[\u0026#34;attn_varentropy\u0026#34;]), 0.1, 1.0) top_k = int(jnp.clip( jnp.round(cfg.top_k * (1 + cfg.ada_top_k_int * metrics[\u0026#34;interaction_strength\u0026#34;].item() - cfg.ada_top_k_agree * metrics[\u0026#34;agreement\u0026#34;].item())), a_min=1, a_max=100 )) min_p = jnp.clip(cfg.min_p * (1 - cfg.ada_min_p * logits_uncertainty), 0.01, 0.5) keys = jax.random.split(key, cfg.n_adaptive_samples) samples = [] for sample_key in keys: sample = _sample(logits, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, key=sample_key) samples.append(sample) def score_sample(sample): log_prob = jnp.sum(jax.nn.log_softmax(logits) * jax.nn.one_hot(sample, logits.shape[-1])) confidence_score = ( (1 - metrics[\u0026#34;logits_entropy\u0026#34;]) * cfg.ada_score_logits_ent + (1 - metrics[\u0026#34;attn_entropy\u0026#34;]) * cfg.ada_score_attn_ent + (1 - metrics[\u0026#34;logits_varentropy\u0026#34;]) * cfg.ada_score_logits_vent + (1 - metrics[\u0026#34;attn_varentropy\u0026#34;]) * cfg.ada_score_attn_vent + metrics[\u0026#34;agreement\u0026#34;] * cfg.ada_score_agree + metrics[\u0026#34;interaction_strength\u0026#34;] * cfg.ada_score_int ) return log_prob + confidence_score sample_scores = [score_sample(sample) for sample in samples] best_sample_idx = jnp.argmax(jnp.array(sample_scores)) return samples[best_sample_idx] å¯èƒ½å¤§å®¶å¯¹clarification insertionæ¯”è¾ƒéš¾ç†è§£ï¼Œè¿™é‡Œä¸¾ä¸€ä¸ªä¾‹å­æ¥è¯´åï¼š\nä¾‹å¦‚\n1 2 3 4 5 6 7 Input: \u0026#34;The best programming language for\u0026#34; ç”¨æˆ·è¾“å…¥çš„è¿™ä¸ªé—®é¢˜å…·æœ‰å¾ˆå¼ºçš„ç–‘æƒ‘ï¼Œä½†æ˜¯ä¸æ˜¯è¯´æ¨¡å‹ä¸ç†è§£ã€‚ ä¾‹å¦‚å¯ä»¥æ˜¯Javaï¼Œpythonç­‰ï¼Œæ¨¡å‹éƒ½å¯ä»¥å›ç­”çš„å¾ˆå¥½ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆè¿™é‡Œçš„æ˜¯**High Entropy, Low Varentropy æ‰€ä»¥ï¼Œæœ€å¥½çš„è§£å†³ç­–ç•¥æ˜¯clarification insertionã€‚å³æ’å…¥ä¸€ä¸ªé—®é¢˜ï¼Œè¿›ä¸€æ­¥è¯´æ˜é—®é¢˜åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ** Output: \u0026#34; [CLARIFY] What specific task or criteria are you considering?\u0026#34; ","permalink":"https://LiuChaoXD.github.io/posts/large-language-models/entropix/","summary":"\u003cp\u003eæœ€è¿‘æ¯”è¾ƒç«çš„é€šè¿‡å¦ä¸€ç§é‡‡æ ·æ–¹æ³•æé«˜LLMçš„Reasoning abilityæˆ–è€…å¹»è§‰ç°è±¡ï¼š \u003cstrong\u003eEntropix\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"åŸºæœ¬æ¦‚å¿µ\"\u003eåŸºæœ¬æ¦‚å¿µ\u003c/h2\u003e\n\u003cp\u003eç°åœ¨å¤§å¤šæ•°æµè¡Œçš„LLMæ¶æ„æ˜¯åŸºäºTransformer architectureçš„ã€‚è¿™ç§æ¶æ„é€šå¸¸æ¥è®²åŒ…å«ä¸€ä¸‹å‡ ä¸ªå…³é”®éƒ¨åˆ†ï¼š\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEmbedding Layerï¼šç”¨æ¥å°†è¾“å…¥çš„tokenè½¬åŒ–ä¸ºvector\u003c/li\u003e\n\u003cli\u003eSelf-Attention Layersï¼šè‡ªæ³¨æ„åŠ›å±‚ï¼Œå°±æ˜¯ç½‘ç»œè‡ªåŠ¨å­¦ä¹ ç”¨æˆ·è¾“å…¥ä¸€æ®µæ–‡æœ¬ä¸­ï¼Œæ‰€æœ‰æ–‡æœ¬ä¹‹é—´çš„å…³ç³»\u003c/li\u003e\n\u003cli\u003eFeed-Forward Layersï¼šç”¨æ¥è½¬åŒ–è‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡º\u003c/li\u003e\n\u003cli\u003eLayer Normalizationï¼šç”¨æ¥ç¨³å®šå­¦ä¹ \u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"mermaid\"\u003e%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%%\ngraph LR\nclassDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff\nclassDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff\nclassDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff\nclassDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff\nclassDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff\n\n    A([Input]) --\u003e  G(Embedding):::blue\n    G --\u003e B(Self-Attention):::pink\n    B --\u003e C(Layer Norm):::orange\n    C --\u003e D(Feed Forward):::red\n    D --\u003e E(Layer Norm):::green\n    E --\u003e F([Output])\u003c/div\u003e\n\n\u003ch3 id=\"llmå¦‚ä½•generate-textæˆ–è€…completion\"\u003eLLMå¦‚ä½•generate textæˆ–è€…completion\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003estep 1:\u003c/strong\u003e input processingï¼ˆè¾“å…¥å¤„ç†ï¼‰ï¼Œå³å°†input textå…ˆè¿›è¡Œtokenizationï¼Œç„¶åé€šè¿‡embeddingå°†å…¶æ˜ å°„åˆ°vectorç©ºé—´ä¸­\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003estep 2\u003c/strong\u003eï¼šForward processingï¼ˆå‰å‘å¤„ç†ï¼‰ï¼Œå³ç»è¿‡embeddingåé€šè¿‡self-attentionï¼Œlayer normï¼Œfeed-forward ç­‰ï¼Œæœ€ç»ˆè·å¾—æ‰€æœ‰ä¸‹ä¸€ä¸ªå¯èƒ½tokençš„logits\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003estep 3\u003c/strong\u003e: Samplingï¼ˆé‡‡æ ·ï¼‰ï¼Œè¿™é‡Œå°±æ˜¯è¯¥ç¯‡æ–‡ç« æŠ€æœ¯çš„å…³æ³¨ç‚¹ã€‚å›è¿‡å¤´æ¥ï¼Œç°åœ¨å¤§å¤šæ•°èƒ½å½±å“é‡‡æ ·ç»“æœçš„å‚æ•°æœ‰temperatureï¼ˆæ¸©åº¦ç³»æ•°ï¼‰ï¼Œtop-pï¼Œtop-k\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003estep 4\u003c/strong\u003e: Repeat ï¼ˆé‡å¤ä¸Šè¿°æ­¥éª¤ï¼‰ï¼šå³å½“é‡‡æ ·å¥½äº†ä¸‹ä¸€ä¸ªtokenåï¼Œä¼šå°†è¯¥tokenæ·»åŠ åˆ°input textçš„æœ«å°¾ï¼Œå³æ­¤æ—¶çš„è¾“å…¥å˜ä¸ºäº†input text + â€œnext tokenâ€ï¼Œç„¶åé‡‡æ ·â€œnext next tokenâ€\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"logitsçš„ä½œç”¨\"\u003eLogitsçš„ä½œç”¨\u003c/h3\u003e\n\u003cp\u003elogitså°±æ˜¯æ¦‚ç‡ï¼Œä¸»è¦æ˜¯é€šè¿‡softmaxå‡½æ•°å»å°†æœ€åä¸€å±‚çš„è¾“å‡ºè½¬åŒ–ä¸ºæ€»å’Œä¸º1çš„æ¦‚ç‡ã€‚å³\u003c/p\u003e","title":"Entropy Based Sampling and Parallel CoT Decoding"},{"content":"This is a tester\n","permalink":"https://LiuChaoXD.github.io/posts/others/test-copy/","summary":"\u003cp\u003eThis is a tester\u003c/p\u003e","title":"Test"},{"content":"This is a tester\n","permalink":"https://LiuChaoXD.github.io/posts/others/test/","summary":"\u003cp\u003eThis is a tester\u003c/p\u003e","title":"Test"},{"content":"å­¦ä¹ å¯ä»¥ä¿æŒå¹´è½»\u0026hellip;\né•¿æœŸåœ¨ä¼ä¸šå†…ä»äº‹researchã€ç®—æ³•ç ”å‘ã€å¼€å‘ç›¸å…³å·¥ä½œã€‚ ç ”ç©¶é¢†åŸŸï¼š Large Scale Image Retrieve Hashing Learning Compute Vision Large Language Models Agent/Workflow development AI-powered Software development æ­£åœ¨åŠªåŠ›è¿½æ±‚ç‹¬ç«‹å¼€å‘çš„è·¯ä¸Š\u0026hellip;. ç‹¬ç«‹å¼€å‘web app PDF2MindMapï¼šå°†sci paperè‡ªåŠ¨è§£æè§£æ„ï¼Œå½’çº³æ•´ç†ä¸ºmindmapçš„å·¥å…· AutoPrompterï¼šè‡ªå®šä¹‰ä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡ï¼Œè‡ªåŠ¨ç¼–å†™é«˜è´¨é‡promptçš„å·¥å…· æœ‰å…³åšå®¢å†…å®¹ã€åˆä½œæ„å‘ï¼Œæ¬¢è¿è”ç³»ã€‚\nè”ç³»æ–¹å¼ï¼špdf2mindmap@gmail.com\n","permalink":"https://LiuChaoXD.github.io/about/","summary":"\u003cp\u003eå­¦ä¹ å¯ä»¥ä¿æŒå¹´è½»\u0026hellip;\u003c/p\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003eé•¿æœŸåœ¨ä¼ä¸šå†…ä»äº‹researchã€ç®—æ³•ç ”å‘ã€å¼€å‘ç›¸å…³å·¥ä½œã€‚\u003c/li\u003e\n\u003cli\u003eç ”ç©¶é¢†åŸŸï¼š\n\u003cul\u003e\n\u003cli\u003eLarge Scale Image Retrieve\u003c/li\u003e\n\u003cli\u003eHashing Learning\u003c/li\u003e\n\u003cli\u003eCompute Vision\u003c/li\u003e\n\u003cli\u003eLarge Language Models\u003c/li\u003e\n\u003cli\u003eAgent/Workflow development\u003c/li\u003e\n\u003cli\u003eAI-powered Software development\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003eæ­£åœ¨åŠªåŠ›è¿½æ±‚ç‹¬ç«‹å¼€å‘çš„è·¯ä¸Š\u0026hellip;.\u003c/li\u003e\n\u003cli\u003eç‹¬ç«‹å¼€å‘web app\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePDF2MindMap\u003c/strong\u003eï¼šå°†sci paperè‡ªåŠ¨è§£æè§£æ„ï¼Œå½’çº³æ•´ç†ä¸ºmindmapçš„å·¥å…·\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAutoPrompter\u003c/strong\u003eï¼šè‡ªå®šä¹‰ä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡ï¼Œè‡ªåŠ¨ç¼–å†™é«˜è´¨é‡promptçš„å·¥å…·\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eæœ‰å…³åšå®¢å†…å®¹ã€åˆä½œæ„å‘ï¼Œæ¬¢è¿è”ç³»ã€‚\u003c/p\u003e\n\u003cp\u003eè”ç³»æ–¹å¼ï¼špdf2mindmap@gmail.com\u003c/p\u003e","title":"About Me"}]