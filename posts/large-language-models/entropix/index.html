<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Entropy Based Sampling and Parallel CoT Decoding | ChaosThoughts.com</title>
<meta name="keywords" content="LLM">
<meta name="description" content="最近比较火的通过另一种采样方法提高LLM的Reasoning ability或者幻觉现象： Entropix
基本概念
现在大多数流行的LLM架构是基于Transformer architecture的。这种架构通常来讲包含一下几个关键部分：

Embedding Layer：用来将输入的token转化为vector
Self-Attention Layers：自注意力层，就是网络自动学习用户输入一段文本中，所有文本之间的关系
Feed-Forward Layers：用来转化自注意力层的输出
Layer Normalization：用来稳定学习

%%{init: {&#39;theme&#39;: &#39;base&#39;, &#39;themeVariables&#39;: { &#39;fontFamily&#39;: &#39;arial&#39;}}}%%
graph LR
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff

    A([Input]) --&gt;  G(Embedding):::blue
    G --&gt; B(Self-Attention):::pink
    B --&gt; C(Layer Norm):::orange
    C --&gt; D(Feed Forward):::red
    D --&gt; E(Layer Norm):::green
    E --&gt; F([Output])
  

LLM如何generate text或者completion

step 1: input processing（输入处理），即将input text先进行tokenization，然后通过embedding将其映射到vector空间中
step 2：Forward processing（前向处理），即经过embedding后通过self-attention，layer norm，feed-forward 等，最终获得所有下一个可能token的logits
step 3: Sampling（采样），这里就是该篇文章技术的关注点。回过头来，现在大多数能影响采样结果的参数有temperature（温度系数），top-p，top-k
step 4: Repeat （重复上述步骤）：即当采样好了下一个token后，会将该token添加到input text的末尾，即此时的输入变为了input text &#43; “next token”，然后采样“next next token”

Logits的作用
logits就是概率，主要是通过softmax函数去将最后一层的输出转化为总和为1的概率。即">
<meta name="author" content="Chao">
<link rel="canonical" href="https://LiuChaoXD.github.io/posts/large-language-models/entropix/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6d8c759a2ca93bab24c74d1df85e8cc155e219ccb27d33f979f405e915112cb9.css" integrity="sha256-bYx1miypO6skx00d&#43;F6MwVXiGcyyfTP5efQF6RURLLk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://LiuChaoXD.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://LiuChaoXD.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://LiuChaoXD.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://LiuChaoXD.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://LiuChaoXD.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://LiuChaoXD.github.io/posts/large-language-models/entropix/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Entropy Based Sampling and Parallel CoT Decoding">
<meta property="og:description" content="最近比较火的通过另一种采样方法提高LLM的Reasoning ability或者幻觉现象： Entropix
基本概念
现在大多数流行的LLM架构是基于Transformer architecture的。这种架构通常来讲包含一下几个关键部分：

Embedding Layer：用来将输入的token转化为vector
Self-Attention Layers：自注意力层，就是网络自动学习用户输入一段文本中，所有文本之间的关系
Feed-Forward Layers：用来转化自注意力层的输出
Layer Normalization：用来稳定学习

%%{init: {&#39;theme&#39;: &#39;base&#39;, &#39;themeVariables&#39;: { &#39;fontFamily&#39;: &#39;arial&#39;}}}%%
graph LR
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff

    A([Input]) --&gt;  G(Embedding):::blue
    G --&gt; B(Self-Attention):::pink
    B --&gt; C(Layer Norm):::orange
    C --&gt; D(Feed Forward):::red
    D --&gt; E(Layer Norm):::green
    E --&gt; F([Output])
  

LLM如何generate text或者completion

step 1: input processing（输入处理），即将input text先进行tokenization，然后通过embedding将其映射到vector空间中
step 2：Forward processing（前向处理），即经过embedding后通过self-attention，layer norm，feed-forward 等，最终获得所有下一个可能token的logits
step 3: Sampling（采样），这里就是该篇文章技术的关注点。回过头来，现在大多数能影响采样结果的参数有temperature（温度系数），top-p，top-k
step 4: Repeat （重复上述步骤）：即当采样好了下一个token后，会将该token添加到input text的末尾，即此时的输入变为了input text &#43; “next token”，然后采样“next next token”

Logits的作用
logits就是概率，主要是通过softmax函数去将最后一层的输出转化为总和为1的概率。即">
<meta property="og:type" content="article">
<meta property="og:url" content="https://LiuChaoXD.github.io/posts/large-language-models/entropix/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2024-11-04T22:30:44+08:00">
<meta property="article:modified_time" content="2024-11-04T22:30:44+08:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Entropy Based Sampling and Parallel CoT Decoding">
<meta name="twitter:description" content="最近比较火的通过另一种采样方法提高LLM的Reasoning ability或者幻觉现象： Entropix
基本概念
现在大多数流行的LLM架构是基于Transformer architecture的。这种架构通常来讲包含一下几个关键部分：

Embedding Layer：用来将输入的token转化为vector
Self-Attention Layers：自注意力层，就是网络自动学习用户输入一段文本中，所有文本之间的关系
Feed-Forward Layers：用来转化自注意力层的输出
Layer Normalization：用来稳定学习

%%{init: {&#39;theme&#39;: &#39;base&#39;, &#39;themeVariables&#39;: { &#39;fontFamily&#39;: &#39;arial&#39;}}}%%
graph LR
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff

    A([Input]) --&gt;  G(Embedding):::blue
    G --&gt; B(Self-Attention):::pink
    B --&gt; C(Layer Norm):::orange
    C --&gt; D(Feed Forward):::red
    D --&gt; E(Layer Norm):::green
    E --&gt; F([Output])
  

LLM如何generate text或者completion

step 1: input processing（输入处理），即将input text先进行tokenization，然后通过embedding将其映射到vector空间中
step 2：Forward processing（前向处理），即经过embedding后通过self-attention，layer norm，feed-forward 等，最终获得所有下一个可能token的logits
step 3: Sampling（采样），这里就是该篇文章技术的关注点。回过头来，现在大多数能影响采样结果的参数有temperature（温度系数），top-p，top-k
step 4: Repeat （重复上述步骤）：即当采样好了下一个token后，会将该token添加到input text的末尾，即此时的输入变为了input text &#43; “next token”，然后采样“next next token”

Logits的作用
logits就是概率，主要是通过softmax函数去将最后一层的输出转化为总和为1的概率。即">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://LiuChaoXD.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Blogs for Large Language Models",
      "item": "https://LiuChaoXD.github.io/posts/large-language-models/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Entropy Based Sampling and Parallel CoT Decoding",
      "item": "https://LiuChaoXD.github.io/posts/large-language-models/entropix/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Entropy Based Sampling and Parallel CoT Decoding",
  "name": "Entropy Based Sampling and Parallel CoT Decoding",
  "description": "最近比较火的通过另一种采样方法提高LLM的Reasoning ability或者幻觉现象： Entropix\n基本概念 现在大多数流行的LLM架构是基于Transformer architecture的。这种架构通常来讲包含一下几个关键部分：\nEmbedding Layer：用来将输入的token转化为vector Self-Attention Layers：自注意力层，就是网络自动学习用户输入一段文本中，所有文本之间的关系 Feed-Forward Layers：用来转化自注意力层的输出 Layer Normalization：用来稳定学习 %%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%% graph LR classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Input]) --\u003e G(Embedding):::blue G --\u003e B(Self-Attention):::pink B --\u003e C(Layer Norm):::orange C --\u003e D(Feed Forward):::red D --\u003e E(Layer Norm):::green E --\u003e F([Output]) LLM如何generate text或者completion step 1: input processing（输入处理），即将input text先进行tokenization，然后通过embedding将其映射到vector空间中 step 2：Forward processing（前向处理），即经过embedding后通过self-attention，layer norm，feed-forward 等，最终获得所有下一个可能token的logits step 3: Sampling（采样），这里就是该篇文章技术的关注点。回过头来，现在大多数能影响采样结果的参数有temperature（温度系数），top-p，top-k step 4: Repeat （重复上述步骤）：即当采样好了下一个token后，会将该token添加到input text的末尾，即此时的输入变为了input text + “next token”，然后采样“next next token” Logits的作用 logits就是概率，主要是通过softmax函数去将最后一层的输出转化为总和为1的概率。即\n",
  "keywords": [
    "LLM"
  ],
  "articleBody": "最近比较火的通过另一种采样方法提高LLM的Reasoning ability或者幻觉现象： Entropix\n基本概念 现在大多数流行的LLM架构是基于Transformer architecture的。这种架构通常来讲包含一下几个关键部分：\nEmbedding Layer：用来将输入的token转化为vector Self-Attention Layers：自注意力层，就是网络自动学习用户输入一段文本中，所有文本之间的关系 Feed-Forward Layers：用来转化自注意力层的输出 Layer Normalization：用来稳定学习 %%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%% graph LR classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Input]) --\u003e G(Embedding):::blue G --\u003e B(Self-Attention):::pink B --\u003e C(Layer Norm):::orange C --\u003e D(Feed Forward):::red D --\u003e E(Layer Norm):::green E --\u003e F([Output]) LLM如何generate text或者completion step 1: input processing（输入处理），即将input text先进行tokenization，然后通过embedding将其映射到vector空间中 step 2：Forward processing（前向处理），即经过embedding后通过self-attention，layer norm，feed-forward 等，最终获得所有下一个可能token的logits step 3: Sampling（采样），这里就是该篇文章技术的关注点。回过头来，现在大多数能影响采样结果的参数有temperature（温度系数），top-p，top-k step 4: Repeat （重复上述步骤）：即当采样好了下一个token后，会将该token添加到input text的末尾，即此时的输入变为了input text + “next token”，然后采样“next next token” Logits的作用 logits就是概率，主要是通过softmax函数去将最后一层的输出转化为总和为1的概率。即\n$ P(token_i) = \\frac{e^{logit_i}}{\\sum_j e^{logit_j}} $\nSelf-Attention的作用 self-attention就是可以让LLM去关注到一段文本中不同部分之间的关系。比如“我今天读了三四篇论文，他们都是关于LLM 推理能力的学术论文”，那么这段文本中“三四篇论文”和“推理能力”以及“学术论文”，他们之间的关系就很密切。self-attention机制可以让LLM去关注到这些元素之间的关系。\n💡 这里可以提出一个假设问题：加入attention内部之间捕捉到的关系，很复杂（比如attention weights分布很散），那是不是可以理解为用户输入的这段文本比较复杂（或者用户输入的文本任务复杂、很难） 这一个假设，就和文章的方法有关～\n语言模型中的Entropy（熵） 熵（Entropy） 熵的定义为\n$$ H = -\\sum_{i} p_i \\log_2(p_i) $$\n$p_i$ 表示第 $i$ 个token的概率。此时假设一种情况：用户输入“我要去读论”， 如果vocabulary size为32000\n如果：模型预测下一个token是“文”的概率为1，其他31999个token概率都是0，则熵为0 如果：模型预测下一个token是“文”的概率为0.1，其他31999个token，分别为 $[0.01, 0.2, 0.0043, \\cdots, 0.21]$，那么按照熵的定义计算下来，此时熵会比较大。 💡 直观理解：\n如果熵小，表示预测下一个token（某个token）的概率很高，其他token的概率很小。模型对预测的结果比较笃定。 如果熵大，模型预测下一个token（某一些tokens）的概率都很高，其他token概率比较小。那么模型对预测的结果就不笃定。 熵的方差（Varentropy） 直观理解就是在一个位置上、预测的token的熵变化有多大。\n具体计算方法为：对于当前位置“我要去读论+当前位置”\n计算该位置的token的概率probability（经过softmax获得的logits），以及log probability 计算熵 计算negative log probability和熵entropy之间的差值的平方 💡 熵的方差：可以直观理解为模型对预测当前位置token的不确定性有多高。方差越大，则模型预测越不确定\n熵和熵的方差具体计算代码如下（来自官方的代码库）：\n1 2 3 4 5 6 7 8 9 10 LN_2 = 0.69314718056 # ln(2) = 1.0 / LOG2_E @jax.jit def calculate_varentropy_logsoftmax(logits: jnp.ndarray, axis: int = -1) -\u003e Tuple[jnp.ndarray, jnp.ndarray]: \"\"\"Calculate the entropy and varentropy of the probability distribution using logsoftmax.\"\"\" log_probs = jax.nn.log_softmax(logits, axis=axis) probs = jnp.exp(log_probs) entropy = -jnp.sum(probs * log_probs, axis=axis) / LN_2 # Convert to base-2 varentropy = jnp.sum(probs * (log_probs / LN_2 + entropy[..., None])**2, axis=axis) return entropy, varentropy 总结几种情况：\nLow Entropy, Low Varentropy: 模型对预测的下一个token，具有很高的confidence和consistency. 这种模式下，可能贪婪采样就比较适合greedy sampling High Entropy, Low Varentropy: 模型对预测的下一个token，具有一致的不确定。这种模式下，可能clarification insertion或者increased exploration比较合适 Low Entropy, High Varentropy: 模型对预测的下一个token，具有多种不同的confidence。这种模式下探索（exploration） sampling比较合适. High Entropy, High Varentropy: 模型对预测的下一个token，不确定也不一致。这种模式下模型具有高度的不一致不确定性，所以可能需要调整一些例如top-p，temperature，top-k参数 语言模型中的Attention Self-Attention 内部 %%{init: {'theme':'base'}}%% graph TD classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Input Tokens]) --\u003e B[Multi-Head Attention]:::blue B --\u003e C[Attention Head 1]:::pink B --\u003e D[Attention Head 2]:::pink B --\u003e E[...]:::pink B --\u003e F[Attention Head N]:::pink C --\u003e G[Concatenate \u0026 Linear Transform]:::orange D --\u003e G E --\u003e G F --\u003e G G --\u003e H([Output]) self-attention中有多头机制，那么这里就要假设两种情况：\n在一个attention头中： 如果attention weights的熵entropy比较大，那么表示模型关注到了许多不同的tokens 如果attention weights的熵entropy比较小，那么表示模型只关注到了几个特别的tokens 在多个attention头中 如果多个头的attention weights非常相近，那么表示模型模型的多个头，同时都关注到了几个特别的token 如果多个头的attention weights差异非常大，那么表示模型的多个头，关注的是不同的tokens 这里就可引出两个概念：Attention Entropy 和Attention Agreement\nAttention Entropy：\n1 2 attention_probs = jax.nn.softmax(attention_scores, axis=-1) attn_entropy = -jnp.sum(attention_probs * jnp.log2(jnp.clip(attention_probs, 1e-10, 1.0)), axis=-1) Attention Agreement\n1 2 mean_attention = jnp.mean(attention_probs, axis=1) agreement = jnp.mean(jnp.abs(attention_probs - mean_attention[:, None, :]), axis=(1, 2)) 💡 高attention entropy，会增加探索（exploration）在采样中的作用 低attention agreement，会需要调整top-p，temperature，top-k参数 不同层Self-Attention：Interaction Strength Interaction Strength的定义为：所有层的attention score的绝对值的和（注意是所有）\n$$ \\text{Interaction Strength} = \\frac{1}{L \\cdot H \\cdot N} \\sum_{l=1}^L \\sum_{h=1}^H \\sum_{i=1}^N \\sum_{j=1}^N |A_{l,h,i,j}| $$\n$L$ ：层数 $H$ ： attention heads的个数 $N$ ： 输入文本转化为token之后的长度 $A_{l,h,i,j}$ ： 表示第 $l$ 层， 第 $h$ 个attention head的，第 $i$ 个位置 和 第 $j$ 个位置的attention score 💡 直观的感受是：如果这个interaction strength越高，则表示文本之间的关系越强烈。此时就需要对sampling策略做一定的调整？\ninteraction strength的计算步骤：\nStep 1：提取所有的attention score，注意是所有\nStep 2： 所有的attention score用绝对值\nStep 3：计算均值\n代码：\n1 interaction_strength = jnp.mean(jnp.abs(attention_scores), axis=(1, 2, 3)) Sampling 策略调整 介绍最终的采样策略之前，先回顾一下前文中的相关参数：\nlogits entropy： 预测下一个tokens的logits的熵 varentropy （variance of logits entropy）：熵的方差 attention entropy ： attention score的熵 attention agreement：多个attention head之间的attention 一致性 interaction strength： 所有层的attention score 粗略的调整可以看如下的图\n%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%% graph LR classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff subgraph Metrics subgraph Uncertainty LU[Logits Uncertainty]:::blue AU[Attention Uncertainty]:::blue end A[Agreement]:::pink IS[Interaction Strength]:::orange end subgraph Sampling Parameters T[Temperature] TP[TOP-P] MP[MIN-P] TK[TOP-K] end LU --\u003e|Increate With| T LU --\u003e|Decrease With| T LU --\u003e|Increase With| TP AU --\u003e|Increase With| TP A --\u003e|Decrease With| T A --\u003e|Decrease With| TP A --\u003e|Decrease With| MP A --\u003e|Increase With| MP IS --\u003e|Increase With| TP IS --\u003e|Increase With| TK style Uncertainty fill:#e6e6e6,stroke:#666,stroke-width:1px %% Color coding for increases and decreases linkStyle 0,2,3,7,8,9 stroke:#FF0000,color:#FF0000 linkStyle 1,4,5,6 stroke:#0000FF,color:#0000FF,stroke-dasharray: 3 3 Temperature 系数调整 %%{init: {'theme':'base'}}%% graph TD classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Logits Uncertainty]):::blue --\u003e D[Temperature]:::green B([Attention Uncertainty]):::blue --\u003e D C([Agreement]):::blue --\u003e D D --\u003e E([Final Temperature]):::orange 温度系数的调整策略： $T = T_{base} * (1 + 0.3 * U_{logits} + 0.2 * U_{attn} - 0.2 * A)$\n其中 $T_{base}$ 就是未经调整或者默认的Temperature， $U_{logits}$ 就是 entropy + varentropy $U_{attn}$ 就是 attention entropy + attention varentropy $A$ 就是 attention agreement TOP-P和TOP-K调整策略 TOP-K 1 top_k_adj = max(5, int(top_k * (1 + 0.3 * interaction_strength - 0.2 * agreement))) TOP-P 1 top_p_adj = jnp.clip(base_top_p * (1 + 0.1 * metrics[\"attn_varentropy\"]), 0.1, 1.0) Minimum Probability Threshold 1 min_p = jnp.clip(base_min_p * (1 - 0.5 * logits_uncertainty), 0.01, 0.5) Implementation entropix会在最终采样前计算各种metrics然后做出适应性的调整，用来改变或者增强采样的确定性。如下图所示\n%%{init: {'theme':'base'}}%% graph TD classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff A([Calculate Metrics]):::blue --\u003e B{Evaluate Entropy and Varentropy}:::pink B --\u003e|Low E, Low V| C[Greedy Sampling]:::orange B --\u003e|High E, Low V| D[Clarification Insertion]:::orange B --\u003e|Low E, High V| E[Exploration Sampling]:::orange B --\u003e|High E, High V| F[High Uncertainty Sampling]:::orange B --\u003e|Moderate Values| G[Adaptive Sampling]:::orange C --\u003e H([Generate Token]):::green D --\u003e H E --\u003e H F --\u003e H G --\u003e H 注意这里的E 和V表示的是根据对next token预测的logits，所获得的entropy和varentropy。即不同策略的trigger是通过logits的熵和熵的方差去做的。然后trigger不同的strategy，采用不同的采样调整策略。\n为了方便理解再次将前文粘贴到这里\n总结几种情况：\nLow Entropy, Low Varentropy: 模型对预测的下一个token，具有很高的confidence和consistency. 这种模式下，可能贪婪采样就比较适合greedy sampling High Entropy, Low Varentropy: 模型对预测的下一个token，具有一致的不确定。这种模式下，可能clarification insertion或者increased exploration比较合适 Low Entropy, High Varentropy: 模型对预测的下一个token，具有多种不同的confidence。这种模式下探索（exploration） sampling比较合适. High Entropy, High Varentropy: 模型对预测的下一个token，不确定也不一致。这种模式下模型具有高度的不一致不确定性，所以可能需要调整一些例如top-p，temperature，top-k参数 Metrics计算 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def calculate_metrics(logits: jnp.ndarray, attention_scores: jnp.ndarray) -\u003e Dict[str, jnp.ndarray]: entropy, varentropy = calculate_varentropy_logsoftmax(logits) // 计算logits的熵和熵的方差 attention_probs = jax.nn.softmax(attention_scores, axis=-1) // 计算通过attention scores获得attention probability，主要用来计算attention entropy和attention varentropy attn_entropy = -jnp.sum(attention_probs * jnp.log2(jnp.clip(attention_probs, 1e-10, 1.0)), axis=-1) attn_varentropy = jnp.var(attn_entropy, axis=-1) mean_attention = jnp.mean(attention_probs, axis=1) // 计算所有attention的均值 agreement = jnp.mean(jnp.abs(attention_probs - mean_attention[:, None, :]), axis=(1, 2)) //计算attention的agreement interaction_strength = jnp.mean(jnp.abs(attention_scores), axis=(1, 2, 3)) // 计算interaction strength return { \"logits_entropy\": jnp.mean(entropy), \"logits_varentropy\": jnp.mean(varentropy), \"attn_entropy\": jnp.mean(attn_entropy), \"attn_varentropy\": jnp.mean(attn_varentropy), \"agreement\": jnp.mean(agreement), \"interaction_strength\": interaction_strength } Sampling代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def sample(gen_tokens: jax.Array, logits: jax.Array, attention_scores: jax.Array, cfg: SamplerConfig, clarifying_question_token: int = 2564, key=jax.random.PRNGKey(1337)) -\u003e jax.Array: metrics = calculate_metrics(logits, attention_scores) **ent**, **vent** = metrics[\"logits_entropy\"], metrics[\"logits_varentropy\"] attn_ent, attn_vent = metrics[\"attn_entropy\"], metrics[\"attn_varentropy\"] agreement = metrics[\"agreement\"] interaction_strength = metrics[\"interaction_strength\"] # Low Entropy, Low Varentropy: \"flowing with unspoken intent\" if **ent** \u003c cfg.low_ent_thresh and **vent** \u003c cfg.low_vent_thresh: //注意这里是通过ent 和 vent去判断采用哪一种adaption策略。 return jnp.argmax(logits[:, -1], axis=-1, keepdims=True).astype(jnp.int32) # High Entropy, Low Varentropy: \"treading carefully, asking clarifying questions\" elif ent \u003e cfg.high_ent_thresh and vent \u003c cfg.low_vent_thresh: //这里需要插入一个clarification问题，问清楚到底用户的问题 # Insert a clarifying question token if not already present if not jnp.isin(gen_tokens[:,-1], clarifying_question_token).any(): return jnp.array([[clarifying_question_token]]) else: # If we've just asked a question, sample with slightly higher temperature temp_adj = cfg.helv_attn_ent_offset + cfg.helv_attn_ent_coef * attn_ent # Increase temperature based on attention entropy return _sample(logits, temperature=min(1.5, cfg.temp * temp_adj), top_p=cfg.top_p, top_k=cfg.top_k, min_p=cfg.min_p, key=key) # Low Entropy, High Varentropy: \"exploring forks in the path\" elif ent \u003c cfg.high_ent_thresh and vent \u003e cfg.high_vent_thresh: temp_adj = cfg.lehv_interaction_strength_offset + cfg.lehv_interaction_strength_coef * interaction_strength # Increase temperature based on interaction strength top_k_adj = max(5, int(cfg.top_k * (1 + 0.5 * (1 - agreement)))) # Increase top_k when agreement is low return _sample(logits, temperature=min(1.5, cfg.temp * temp_adj), top_p=cfg.top_p, top_k=top_k_adj, min_p=cfg.min_p, key=key) # High Entropy, High Varentropy: \"resampling in the mist\" elif ent \u003e cfg.med_ent_thresh and vent \u003e cfg.high_vent_thresh: # Use high temperature and adjusted top_p based on attention metrics temp_adj = cfg.hehv_attn_vent_offset + cfg.hehv_attn_vent_coef * attn_vent # Increase temperature based on attention varentropy top_p_adj = max(0.5, cfg.top_p - cfg.hehv_attn_ent_coef * attn_ent) # Decrease top_p when attention entropy is high return _sample(logits, temperature=max(2.0, cfg.temp * temp_adj), top_p=top_p_adj, top_k=cfg.top_k, min_p=cfg.min_p, key=key) # Middle ground: use adaptive sampling else: logits_uncertainty = metrics[\"logits_entropy\"] + metrics[\"logits_varentropy\"] attn_uncertainty = metrics[\"attn_entropy\"] + metrics[\"attn_varentropy\"] temperature = cfg.temp * (1 + cfg.ada_temp_logits * logits_uncertainty + cfg.ada_temp_attn * attn_uncertainty - cfg.ada_temp_agree * metrics[\"agreement\"]) top_p = jnp.clip(cfg.top_p * (1 + cfg.ada_top_p * metrics[\"attn_varentropy\"]), 0.1, 1.0) top_k = int(jnp.clip( jnp.round(cfg.top_k * (1 + cfg.ada_top_k_int * metrics[\"interaction_strength\"].item() - cfg.ada_top_k_agree * metrics[\"agreement\"].item())), a_min=1, a_max=100 )) min_p = jnp.clip(cfg.min_p * (1 - cfg.ada_min_p * logits_uncertainty), 0.01, 0.5) keys = jax.random.split(key, cfg.n_adaptive_samples) samples = [] for sample_key in keys: sample = _sample(logits, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, key=sample_key) samples.append(sample) def score_sample(sample): log_prob = jnp.sum(jax.nn.log_softmax(logits) * jax.nn.one_hot(sample, logits.shape[-1])) confidence_score = ( (1 - metrics[\"logits_entropy\"]) * cfg.ada_score_logits_ent + (1 - metrics[\"attn_entropy\"]) * cfg.ada_score_attn_ent + (1 - metrics[\"logits_varentropy\"]) * cfg.ada_score_logits_vent + (1 - metrics[\"attn_varentropy\"]) * cfg.ada_score_attn_vent + metrics[\"agreement\"] * cfg.ada_score_agree + metrics[\"interaction_strength\"] * cfg.ada_score_int ) return log_prob + confidence_score sample_scores = [score_sample(sample) for sample in samples] best_sample_idx = jnp.argmax(jnp.array(sample_scores)) return samples[best_sample_idx] 可能大家对clarification insertion比较难理解，这里举一个例子来说名：\n例如\n1 2 3 4 5 6 7 Input: \"The best programming language for\" 用户输入的这个问题具有很强的疑惑，但是不是说模型不理解。 例如可以是Java，python等，模型都可以回答的很好。这也是为什么这里的是**High Entropy, Low Varentropy 所以，最好的解决策略是clarification insertion。即插入一个问题，进一步说明问题到底是什么？** Output: \" [CLARIFY] What specific task or criteria are you considering?\" ",
  "wordCount" : "1394",
  "inLanguage": "en",
  "datePublished": "2024-11-04T22:30:44+08:00",
  "dateModified": "2024-11-04T22:30:44+08:00",
  "author":{
    "@type": "Person",
    "name": "Chao"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://LiuChaoXD.github.io/posts/large-language-models/entropix/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ChaosThoughts.com",
    "logo": {
      "@type": "ImageObject",
      "url": "https://LiuChaoXD.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://LiuChaoXD.github.io/" accesskey="h" title="ChaosThoughts.com (Alt + H)">ChaosThoughts.com</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://LiuChaoXD.github.io/" title="🏠Home">
                    <span>🏠Home</span>
                </a>
            </li>
            <li>
                <a href="https://LiuChaoXD.github.io/posts" title="📚Blogs">
                    <span>📚Blogs</span>
                </a>
            </li>
            <li>
                <a href="https://LiuChaoXD.github.io/archives/" title="⏱Archives">
                    <span>⏱Archives</span>
                </a>
            </li>
            <li>
                <a href="https://LiuChaoXD.github.io/tags" title="🔖Tags">
                    <span>🔖Tags</span>
                </a>
            </li>
            <li>
                <a href="https://LiuChaoXD.github.io/search" title="🔍Search (Alt &#43; /)" accesskey=/>
                    <span>🔍Search</span>
                </a>
            </li>
            <li>
                <a href="https://LiuChaoXD.github.io/about" title="🙋🏻‍♂️About">
                    <span>🙋🏻‍♂️About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://LiuChaoXD.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://LiuChaoXD.github.io/posts/">Posts</a>&nbsp;»&nbsp;<a href="https://LiuChaoXD.github.io/posts/large-language-models/">Blogs for Large Language Models</a></div>
    <h1 class="post-title entry-hint-parent">
      Entropy Based Sampling and Parallel CoT Decoding
    </h1>
    <div class="post-meta"><span title='2024-11-04 22:30:44 +0800 CST'>2024-11-04</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Chao

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5" aria-label="基本概念">基本概念</a><ul>
                        
                <li>
                    <a href="#llm%e5%a6%82%e4%bd%95generate-text%e6%88%96%e8%80%85completion" aria-label="LLM如何generate text或者completion">LLM如何generate text或者completion</a></li>
                <li>
                    <a href="#logits%e7%9a%84%e4%bd%9c%e7%94%a8" aria-label="Logits的作用">Logits的作用</a></li>
                <li>
                    <a href="#self-attention%e7%9a%84%e4%bd%9c%e7%94%a8" aria-label="Self-Attention的作用">Self-Attention的作用</a></li></ul>
                </li>
                <li>
                    <a href="#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%b8%ad%e7%9a%84entropy%e7%86%b5" aria-label="语言模型中的Entropy（熵）">语言模型中的Entropy（熵）</a><ul>
                        
                <li>
                    <a href="#%e7%86%b5entropy" aria-label="熵（Entropy）">熵（Entropy）</a></li>
                <li>
                    <a href="#%e7%86%b5%e7%9a%84%e6%96%b9%e5%b7%aevarentropy" aria-label="熵的方差（Varentropy）">熵的方差（Varentropy）</a></li></ul>
                </li>
                <li>
                    <a href="#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%b8%ad%e7%9a%84attention" aria-label="语言模型中的Attention">语言模型中的Attention</a><ul>
                        
                <li>
                    <a href="#self-attention-%e5%86%85%e9%83%a8" aria-label="Self-Attention 内部">Self-Attention 内部</a></li>
                <li>
                    <a href="#%e4%b8%8d%e5%90%8c%e5%b1%82self-attentioninteraction-strength" aria-label="不同层Self-Attention：Interaction Strength">不同层Self-Attention：Interaction Strength</a></li></ul>
                </li>
                <li>
                    <a href="#sampling-%e7%ad%96%e7%95%a5%e8%b0%83%e6%95%b4" aria-label="Sampling 策略调整">Sampling 策略调整</a><ul>
                        
                <li>
                    <a href="#temperature-%e7%b3%bb%e6%95%b0%e8%b0%83%e6%95%b4" aria-label="Temperature 系数调整">Temperature 系数调整</a></li>
                <li>
                    <a href="#top-p%e5%92%8ctop-k%e8%b0%83%e6%95%b4%e7%ad%96%e7%95%a5" aria-label="TOP-P和TOP-K调整策略">TOP-P和TOP-K调整策略</a></li>
                <li>
                    <a href="#minimum-probability-threshold" aria-label="Minimum Probability Threshold">Minimum Probability Threshold</a></li></ul>
                </li>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a><ul>
                        
                <li>
                    <a href="#metrics%e8%ae%a1%e7%ae%97" aria-label="Metrics计算">Metrics计算</a></li>
                <li>
                    <a href="#sampling%e4%bb%a3%e7%a0%81" aria-label="Sampling代码">Sampling代码</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>最近比较火的通过另一种采样方法提高LLM的Reasoning ability或者幻觉现象： <strong>Entropix</strong></p>
<h2 id="基本概念">基本概念<a hidden class="anchor" aria-hidden="true" href="#基本概念">#</a></h2>
<p>现在大多数流行的LLM架构是基于Transformer architecture的。这种架构通常来讲包含一下几个关键部分：</p>
<ol>
<li>Embedding Layer：用来将输入的token转化为vector</li>
<li>Self-Attention Layers：自注意力层，就是网络自动学习用户输入一段文本中，所有文本之间的关系</li>
<li>Feed-Forward Layers：用来转化自注意力层的输出</li>
<li>Layer Normalization：用来稳定学习</li>
</ol>
<pre class="mermaid">%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%%
graph LR
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff

    A([Input]) -->  G(Embedding):::blue
    G --> B(Self-Attention):::pink
    B --> C(Layer Norm):::orange
    C --> D(Feed Forward):::red
    D --> E(Layer Norm):::green
    E --> F([Output])
  </pre
>

<h3 id="llm如何generate-text或者completion">LLM如何generate text或者completion<a hidden class="anchor" aria-hidden="true" href="#llm如何generate-text或者completion">#</a></h3>
<ul>
<li><strong>step 1:</strong> input processing（输入处理），即将input text先进行tokenization，然后通过embedding将其映射到vector空间中</li>
<li><strong>step 2</strong>：Forward processing（前向处理），即经过embedding后通过self-attention，layer norm，feed-forward 等，最终获得所有下一个可能token的logits</li>
<li><strong>step 3</strong>: Sampling（采样），这里就是该篇文章技术的关注点。回过头来，现在大多数能影响采样结果的参数有temperature（温度系数），top-p，top-k</li>
<li><strong>step 4</strong>: Repeat （重复上述步骤）：即当采样好了下一个token后，会将该token添加到input text的末尾，即此时的输入变为了input text + “next token”，然后采样“next next token”</li>
</ul>
<h3 id="logits的作用">Logits的作用<a hidden class="anchor" aria-hidden="true" href="#logits的作用">#</a></h3>
<p>logits就是概率，主要是通过softmax函数去将最后一层的输出转化为总和为1的概率。即</p>
<p>$ P(token_i) = \frac{e^{logit_i}}{\sum_j e^{logit_j}} $</p>
<h3 id="self-attention的作用">Self-Attention的作用<a hidden class="anchor" aria-hidden="true" href="#self-attention的作用">#</a></h3>
<p>self-attention就是可以让LLM去关注到一段文本中不同部分之间的关系。比如“我今天读了三四篇论文，他们都是关于LLM 推理能力的学术论文”，那么这段文本中“三四篇论文”和“推理能力”以及“学术论文”，他们之间的关系就很密切。self-attention机制可以让LLM去关注到这些元素之间的关系。</p>
<blockquote>
<p>💡
这里可以提出一个假设问题：加入attention内部之间捕捉到的关系，很复杂（比如attention weights分布很散），那是不是可以理解为用户输入的这段文本比较复杂（或者用户输入的文本任务复杂、很难）
这一个假设，就和文章的方法有关～</p>
</blockquote>
<h2 id="语言模型中的entropy熵">语言模型中的Entropy（熵）<a hidden class="anchor" aria-hidden="true" href="#语言模型中的entropy熵">#</a></h2>
<h3 id="熵entropy">熵（Entropy）<a hidden class="anchor" aria-hidden="true" href="#熵entropy">#</a></h3>
<p>熵的定义为</p>
<p>$$
H = -\sum_{i} p_i \log_2(p_i)
$$</p>
<p>$p_i$  表示第 $i$ 个token的概率。此时假设一种情况：用户输入“我要去读论”， 如果vocabulary size为32000</p>
<ol>
<li>如果：模型预测下一个token是“文”的概率为1，其他31999个token概率都是0，则熵为0</li>
<li>如果：模型预测下一个token是“文”的概率为0.1，其他31999个token，分别为 $[0.01, 0.2, 0.0043, \cdots, 0.21]$，那么按照熵的定义计算下来，此时熵会比较大。</li>
</ol>
<aside>
💡
<p>直观理解：</p>
<ol>
<li>如果熵小，表示预测下一个token（某个token）的概率很高，其他token的概率很小。模型对预测的结果比较笃定。</li>
<li>如果熵大，模型预测下一个token（某一些tokens）的概率都很高，其他token概率比较小。那么模型对预测的结果就不笃定。</li>
</ol>
</aside>
<h3 id="熵的方差varentropy">熵的方差（<strong>Varentropy</strong>）<a hidden class="anchor" aria-hidden="true" href="#熵的方差varentropy">#</a></h3>
<p>直观理解就是在一个位置上、预测的token的熵变化有多大。</p>
<p>具体计算方法为：对于当前位置“我要去读论+当前位置”</p>
<ol>
<li>计算该位置的token的概率probability（经过softmax获得的logits），以及log probability</li>
<li>计算熵</li>
<li>计算negative log probability和熵entropy之间的差值的平方</li>
</ol>
<aside>
💡
<p>熵的方差：可以直观理解为模型对预测当前位置token的不确定性有多高。方差越大，则模型预测越不确定</p>
</aside>
<p>熵和熵的方差具体计算代码如下（来自官方的代码库）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-1-1"><a class="lnlinks" href="#hl-1-1"> 1</a>
</span><span class="lnt" id="hl-1-2"><a class="lnlinks" href="#hl-1-2"> 2</a>
</span><span class="lnt" id="hl-1-3"><a class="lnlinks" href="#hl-1-3"> 3</a>
</span><span class="lnt" id="hl-1-4"><a class="lnlinks" href="#hl-1-4"> 4</a>
</span><span class="lnt" id="hl-1-5"><a class="lnlinks" href="#hl-1-5"> 5</a>
</span><span class="lnt" id="hl-1-6"><a class="lnlinks" href="#hl-1-6"> 6</a>
</span><span class="lnt" id="hl-1-7"><a class="lnlinks" href="#hl-1-7"> 7</a>
</span><span class="lnt" id="hl-1-8"><a class="lnlinks" href="#hl-1-8"> 8</a>
</span><span class="lnt" id="hl-1-9"><a class="lnlinks" href="#hl-1-9"> 9</a>
</span><span class="lnt" id="hl-1-10"><a class="lnlinks" href="#hl-1-10">10</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">LN_2</span> <span class="o">=</span> <span class="mf">0.69314718056</span>  <span class="c1"># ln(2) = 1.0 / LOG2_E</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@jax.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_varentropy_logsoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Calculate the entropy and varentropy of the probability distribution using logsoftmax.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">probs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probs</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span> <span class="o">/</span> <span class="n">LN_2</span>  <span class="c1"># Convert to base-2</span>
</span></span><span class="line"><span class="cl">    <span class="n">varentropy</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probs</span> <span class="o">*</span> <span class="p">(</span><span class="n">log_probs</span> <span class="o">/</span> <span class="n">LN_2</span> <span class="o">+</span> <span class="n">entropy</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">varentropy</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>总结几种情况：</p>
<p><img alt="<strong>Low Entropy, Low Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.12.25.png"></p>
<!-- **Low Entropy, Low Varentropy** -->
<p><img alt="<strong>High Entropy, Low Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.12.51.png"></p>
<!-- **High Entropy, Low Varentropy** -->
<p><img alt="<strong>Low Entropy, High Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.13.04.png"></p>
<!-- **Low Entropy, High Varentropy** -->
<p><img alt="<strong>High Entropy, High Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.13.14.png"></p>
<!-- **High Entropy, High Varentropy** -->
<ol>
<li><strong>Low Entropy, Low Varentropy: 模型对预测的下一个token，具有很高的confidence和consistency. 这种模式下，可能贪婪采样就比较适合greedy sampling</strong></li>
<li><strong>High Entropy, Low Varentropy: 模型对预测的下一个token，具有一致的不确定。这种模式下，可能clarification insertion或者increased exploration比较合适</strong></li>
<li><strong>Low Entropy, High Varentropy: 模型对预测的下一个token，具有多种不同的confidence。这种模式下探索（exploration） sampling比较合适.</strong></li>
<li><strong>High Entropy, High Varentropy: 模型对预测的下一个token，不确定也不一致。这种模式下模型具有高度的不一致不确定性，所以可能需要调整一些例如top-p，temperature，top-k参数</strong></li>
</ol>
<h2 id="语言模型中的attention">语言模型中的Attention<a hidden class="anchor" aria-hidden="true" href="#语言模型中的attention">#</a></h2>
<h3 id="self-attention-内部">Self-Attention 内部<a hidden class="anchor" aria-hidden="true" href="#self-attention-内部">#</a></h3>
<pre class="mermaid">%%{init: {'theme':'base'}}%%
graph TD
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff
    A([Input Tokens]) --> B[Multi-Head Attention]:::blue
    B --> C[Attention Head 1]:::pink
    B --> D[Attention Head 2]:::pink
    B --> E[...]:::pink
    B --> F[Attention Head N]:::pink
    C --> G[Concatenate & Linear Transform]:::orange
    D --> G
    E --> G
    F --> G
    G --> H([Output])
  </pre
>

<p>self-attention中有多头机制，那么这里就要假设两种情况：</p>
<ol>
<li>在一个attention头中：
<ol>
<li>如果attention weights的熵entropy比较大，那么表示模型关注到了许多不同的tokens</li>
<li>如果attention weights的熵entropy比较小，那么表示模型只关注到了几个特别的tokens</li>
</ol>
</li>
<li>在多个attention头中
<ol>
<li>如果多个头的attention weights非常相近，那么表示模型模型的多个头，同时都关注到了几个特别的token</li>
<li>如果多个头的attention weights差异非常大，那么表示模型的多个头，关注的是不同的tokens</li>
</ol>
</li>
</ol>
<p>这里就可引出两个概念：Attention Entropy 和Attention Agreement</p>
<p>Attention Entropy：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-3-1"><a class="lnlinks" href="#hl-3-1">1</a>
</span><span class="lnt" id="hl-3-2"><a class="lnlinks" href="#hl-3-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">attn_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_probs</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Attention Agreement</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-4-1"><a class="lnlinks" href="#hl-4-1">1</a>
</span><span class="lnt" id="hl-4-2"><a class="lnlinks" href="#hl-4-2">2</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mean_attention</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">agreement</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attention_probs</span> <span class="o">-</span> <span class="n">mean_attention</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><aside>
💡
<ol>
<li>高attention entropy，会增加探索（exploration）在采样中的作用</li>
<li>低attention agreement，会需要调整<strong>top-p，temperature，top-k参数</strong></li>
</ol>
</aside>
<h3 id="不同层self-attentioninteraction-strength">不同层Self-Attention：Interaction Strength<a hidden class="anchor" aria-hidden="true" href="#不同层self-attentioninteraction-strength">#</a></h3>
<p>Interaction Strength的定义为：所有层的attention score的绝对值的和（注意是所有）</p>
<p>$$
\text{Interaction Strength} = \frac{1}{L \cdot H \cdot N} \sum_{l=1}^L \sum_{h=1}^H \sum_{i=1}^N \sum_{j=1}^N |A_{l,h,i,j}|
$$</p>
<ul>
<li>$L$  ：层数</li>
<li>$H$ ： attention heads的个数</li>
<li>$N$  ： 输入文本转化为token之后的长度</li>
<li>$A_{l,h,i,j}$  ： 表示第 $l$ 层， 第 $h$ 个attention head的，第 $i$ 个位置 和 第 $j$ 个位置的attention score</li>
</ul>
<aside>
💡
<p>直观的感受是：如果这个interaction strength越高，则表示文本之间的关系越强烈。此时就需要对sampling策略做一定的调整？</p>
</aside>
<p>interaction strength的计算步骤：</p>
<p>Step 1：提取所有的attention score，注意是所有</p>
<p>Step 2： 所有的attention score用绝对值</p>
<p>Step 3：计算均值</p>
<p>代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-5-1"><a class="lnlinks" href="#hl-5-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">interaction_strength</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="sampling-策略调整">Sampling 策略调整<a hidden class="anchor" aria-hidden="true" href="#sampling-策略调整">#</a></h2>
<p>介绍最终的采样策略之前，先回顾一下前文中的相关参数：</p>
<ol>
<li>logits entropy： 预测下一个tokens的logits的熵</li>
<li>varentropy （variance of logits entropy）：熵的方差</li>
<li>attention entropy ： attention score的熵</li>
<li>attention agreement：多个attention head之间的attention 一致性</li>
<li>interaction strength： 所有层的attention score</li>
</ol>
<p>粗略的调整可以看如下的图</p>
<pre class="mermaid">%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'arial'}}}%%

graph LR
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff
    subgraph Metrics
        subgraph Uncertainty
            LU[Logits Uncertainty]:::blue 
            AU[Attention Uncertainty]:::blue 
        end
        A[Agreement]:::pink 
        IS[Interaction Strength]:::orange 
    end
    
    subgraph Sampling Parameters
        T[Temperature]
        TP[TOP-P]
        MP[MIN-P]
        TK[TOP-K]
    end
    
    LU -->|Increate With| T
    LU -->|Decrease With| T
    LU -->|Increase With| TP
    
    AU -->|Increase With| TP
    
    A -->|Decrease With| T
    A -->|Decrease With| TP
    A -->|Decrease With| MP
    A -->|Increase With| MP
    
    IS -->|Increase With| TP
    IS -->|Increase With| TK
    
    style Uncertainty fill:#e6e6e6,stroke:#666,stroke-width:1px
    
    %% Color coding for increases and decreases
    linkStyle 0,2,3,7,8,9 stroke:#FF0000,color:#FF0000
    linkStyle 1,4,5,6 stroke:#0000FF,color:#0000FF,stroke-dasharray: 3 3
    
    
  </pre
>

<h3 id="temperature-系数调整">Temperature 系数调整<a hidden class="anchor" aria-hidden="true" href="#temperature-系数调整">#</a></h3>
<pre class="mermaid">%%{init: {'theme':'base'}}%%
graph TD
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff
    A([Logits Uncertainty]):::blue  --> D[Temperature]:::green
    B([Attention Uncertainty]):::blue --> D
    C([Agreement]):::blue --> D
    D --> E([Final Temperature]):::orange
  </pre
>

<p>温度系数的调整策略： $T = T_{base} * (1 + 0.3 * U_{logits} + 0.2 * U_{attn} - 0.2 * A)$</p>
<ul>
<li>其中 $T_{base}$ 就是未经调整或者默认的Temperature，</li>
<li>$U_{logits}$ 就是 entropy + varentropy</li>
<li>$U_{attn}$ 就是 attention entropy + attention varentropy</li>
<li>$A$ 就是 attention agreement</li>
</ul>
<h3 id="top-p和top-k调整策略">TOP-P和TOP-K调整策略<a hidden class="anchor" aria-hidden="true" href="#top-p和top-k调整策略">#</a></h3>
<ul>
<li>TOP-K</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-8-1"><a class="lnlinks" href="#hl-8-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">top_k_adj</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">top_k</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">interaction_strength</span> <span class="o">-</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">agreement</span><span class="p">)))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>TOP-P</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-9-1"><a class="lnlinks" href="#hl-9-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">top_p_adj</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">base_top_p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_varentropy&#34;</span><span class="p">]),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="minimum-probability-threshold">Minimum Probability Threshold<a hidden class="anchor" aria-hidden="true" href="#minimum-probability-threshold">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-10-1"><a class="lnlinks" href="#hl-10-1">1</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">min_p</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">base_min_p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">logits_uncertainty</span><span class="p">),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h2>
<p>entropix会在最终采样前计算各种metrics然后做出适应性的调整，用来改变或者增强采样的确定性。如下图所示</p>
<pre class="mermaid">%%{init: {'theme':'base'}}%%
graph TD
classDef blue fill:#2374f7,stroke:#000,stroke-width:2px,color:#fff
classDef pink fill:#eb3dd6,stroke:#000,stroke-width:2px,color:#fff
classDef orange fill:#fc822b,stroke:#000,stroke-width:2px,color:#fff
classDef red fill:#ed2633,stroke:#000,stroke-width:2px,color:#fff
classDef green fill:#16b522,stroke:#000,stroke-width:2px,color:#fff
    A([Calculate Metrics]):::blue --> B{Evaluate Entropy and Varentropy}:::pink
    B -->|Low E, Low V| C[Greedy Sampling]:::orange
    B -->|High E, Low V| D[Clarification Insertion]:::orange
    B -->|Low E, High V| E[Exploration Sampling]:::orange
    B -->|High E, High V| F[High Uncertainty Sampling]:::orange
    B -->|Moderate Values| G[Adaptive Sampling]:::orange
    C --> H([Generate Token]):::green
    D --> H
    E --> H
    F --> H
    G --> H
  </pre
>

<p><strong>注意这里的E 和V表示的是根据对next token预测的logits，所获得的entropy和varentropy。即不同策略的trigger是通过logits的熵和熵的方差去做的。然后trigger不同的strategy，采用不同的采样调整策略。</strong></p>
<p>为了方便理解再次将前文粘贴到这里</p>
<p>总结几种情况：</p>
<p><img alt="<strong>Low Entropy, Low Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.12.25.png"></p>
<!-- **Low Entropy, Low Varentropy** -->
<p><img alt="<strong>High Entropy, Low Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.12.51.png"></p>
<!-- **High Entropy, Low Varentropy** -->
<p><img alt="<strong>Low Entropy, High Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.13.04.png"></p>
<!-- **Low Entropy, High Varentropy** -->
<p><img alt="<strong>High Entropy, High Varentropy</strong>" loading="lazy" src="/posts/large-language-models/entropix/Screenshot_2024-10-15_at_22.13.14.png"></p>
<ol>
<li><strong>Low Entropy, Low Varentropy: 模型对预测的下一个token，具有很高的confidence和consistency. 这种模式下，可能贪婪采样就比较适合greedy sampling</strong></li>
<li><strong>High Entropy, Low Varentropy: 模型对预测的下一个token，具有一致的不确定。这种模式下，可能clarification insertion或者increased exploration比较合适</strong></li>
<li><strong>Low Entropy, High Varentropy: 模型对预测的下一个token，具有多种不同的confidence。这种模式下探索（exploration） sampling比较合适.</strong></li>
<li><strong>High Entropy, High Varentropy: 模型对预测的下一个token，不确定也不一致。这种模式下模型具有高度的不一致不确定性，所以可能需要调整一些例如top-p，temperature，top-k参数</strong></li>
</ol>
<h3 id="metrics计算">Metrics计算<a hidden class="anchor" aria-hidden="true" href="#metrics计算">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-12-1"><a class="lnlinks" href="#hl-12-1"> 1</a>
</span><span class="lnt" id="hl-12-2"><a class="lnlinks" href="#hl-12-2"> 2</a>
</span><span class="lnt" id="hl-12-3"><a class="lnlinks" href="#hl-12-3"> 3</a>
</span><span class="lnt" id="hl-12-4"><a class="lnlinks" href="#hl-12-4"> 4</a>
</span><span class="lnt" id="hl-12-5"><a class="lnlinks" href="#hl-12-5"> 5</a>
</span><span class="lnt" id="hl-12-6"><a class="lnlinks" href="#hl-12-6"> 6</a>
</span><span class="lnt" id="hl-12-7"><a class="lnlinks" href="#hl-12-7"> 7</a>
</span><span class="lnt" id="hl-12-8"><a class="lnlinks" href="#hl-12-8"> 8</a>
</span><span class="lnt" id="hl-12-9"><a class="lnlinks" href="#hl-12-9"> 9</a>
</span><span class="lnt" id="hl-12-10"><a class="lnlinks" href="#hl-12-10">10</a>
</span><span class="lnt" id="hl-12-11"><a class="lnlinks" href="#hl-12-11">11</a>
</span><span class="lnt" id="hl-12-12"><a class="lnlinks" href="#hl-12-12">12</a>
</span><span class="lnt" id="hl-12-13"><a class="lnlinks" href="#hl-12-13">13</a>
</span><span class="lnt" id="hl-12-14"><a class="lnlinks" href="#hl-12-14">14</a>
</span><span class="lnt" id="hl-12-15"><a class="lnlinks" href="#hl-12-15">15</a>
</span><span class="lnt" id="hl-12-16"><a class="lnlinks" href="#hl-12-16">16</a>
</span><span class="lnt" id="hl-12-17"><a class="lnlinks" href="#hl-12-17">17</a>
</span><span class="lnt" id="hl-12-18"><a class="lnlinks" href="#hl-12-18">18</a>
</span><span class="lnt" id="hl-12-19"><a class="lnlinks" href="#hl-12-19">19</a>
</span><span class="lnt" id="hl-12-20"><a class="lnlinks" href="#hl-12-20">20</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_metrics</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">attention_scores</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">    <span class="n">entropy</span><span class="p">,</span> <span class="n">varentropy</span> <span class="o">=</span> <span class="n">calculate_varentropy_logsoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">//</span> <span class="n">计算logits的熵和熵的方差</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">计算通过attention</span> <span class="n">scores获得attention</span> <span class="n">probability</span><span class="err">，</span><span class="n">主要用来计算attention</span> <span class="n">entropy和attention</span> <span class="n">varentropy</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_probs</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_varentropy</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">attn_entropy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mean_attention</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">计算所有attention的均值</span>
</span></span><span class="line"><span class="cl">    <span class="n">agreement</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attention_probs</span> <span class="o">-</span> <span class="n">mean_attention</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">//</span><span class="n">计算attention的agreement</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">interaction_strength</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">//</span> <span class="n">计算interaction</span> <span class="n">strength</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;logits_entropy&#34;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">entropy</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;logits_varentropy&#34;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">varentropy</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;attn_entropy&#34;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">attn_entropy</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;attn_varentropy&#34;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">attn_varentropy</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;agreement&#34;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">agreement</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;interaction_strength&#34;</span><span class="p">:</span> <span class="n">interaction_strength</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="sampling代码">Sampling代码<a hidden class="anchor" aria-hidden="true" href="#sampling代码">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-13-1"><a class="lnlinks" href="#hl-13-1"> 1</a>
</span><span class="lnt" id="hl-13-2"><a class="lnlinks" href="#hl-13-2"> 2</a>
</span><span class="lnt" id="hl-13-3"><a class="lnlinks" href="#hl-13-3"> 3</a>
</span><span class="lnt" id="hl-13-4"><a class="lnlinks" href="#hl-13-4"> 4</a>
</span><span class="lnt" id="hl-13-5"><a class="lnlinks" href="#hl-13-5"> 5</a>
</span><span class="lnt" id="hl-13-6"><a class="lnlinks" href="#hl-13-6"> 6</a>
</span><span class="lnt" id="hl-13-7"><a class="lnlinks" href="#hl-13-7"> 7</a>
</span><span class="lnt" id="hl-13-8"><a class="lnlinks" href="#hl-13-8"> 8</a>
</span><span class="lnt" id="hl-13-9"><a class="lnlinks" href="#hl-13-9"> 9</a>
</span><span class="lnt" id="hl-13-10"><a class="lnlinks" href="#hl-13-10">10</a>
</span><span class="lnt" id="hl-13-11"><a class="lnlinks" href="#hl-13-11">11</a>
</span><span class="lnt" id="hl-13-12"><a class="lnlinks" href="#hl-13-12">12</a>
</span><span class="lnt" id="hl-13-13"><a class="lnlinks" href="#hl-13-13">13</a>
</span><span class="lnt" id="hl-13-14"><a class="lnlinks" href="#hl-13-14">14</a>
</span><span class="lnt" id="hl-13-15"><a class="lnlinks" href="#hl-13-15">15</a>
</span><span class="lnt" id="hl-13-16"><a class="lnlinks" href="#hl-13-16">16</a>
</span><span class="lnt" id="hl-13-17"><a class="lnlinks" href="#hl-13-17">17</a>
</span><span class="lnt" id="hl-13-18"><a class="lnlinks" href="#hl-13-18">18</a>
</span><span class="lnt" id="hl-13-19"><a class="lnlinks" href="#hl-13-19">19</a>
</span><span class="lnt" id="hl-13-20"><a class="lnlinks" href="#hl-13-20">20</a>
</span><span class="lnt" id="hl-13-21"><a class="lnlinks" href="#hl-13-21">21</a>
</span><span class="lnt" id="hl-13-22"><a class="lnlinks" href="#hl-13-22">22</a>
</span><span class="lnt" id="hl-13-23"><a class="lnlinks" href="#hl-13-23">23</a>
</span><span class="lnt" id="hl-13-24"><a class="lnlinks" href="#hl-13-24">24</a>
</span><span class="lnt" id="hl-13-25"><a class="lnlinks" href="#hl-13-25">25</a>
</span><span class="lnt" id="hl-13-26"><a class="lnlinks" href="#hl-13-26">26</a>
</span><span class="lnt" id="hl-13-27"><a class="lnlinks" href="#hl-13-27">27</a>
</span><span class="lnt" id="hl-13-28"><a class="lnlinks" href="#hl-13-28">28</a>
</span><span class="lnt" id="hl-13-29"><a class="lnlinks" href="#hl-13-29">29</a>
</span><span class="lnt" id="hl-13-30"><a class="lnlinks" href="#hl-13-30">30</a>
</span><span class="lnt" id="hl-13-31"><a class="lnlinks" href="#hl-13-31">31</a>
</span><span class="lnt" id="hl-13-32"><a class="lnlinks" href="#hl-13-32">32</a>
</span><span class="lnt" id="hl-13-33"><a class="lnlinks" href="#hl-13-33">33</a>
</span><span class="lnt" id="hl-13-34"><a class="lnlinks" href="#hl-13-34">34</a>
</span><span class="lnt" id="hl-13-35"><a class="lnlinks" href="#hl-13-35">35</a>
</span><span class="lnt" id="hl-13-36"><a class="lnlinks" href="#hl-13-36">36</a>
</span><span class="lnt" id="hl-13-37"><a class="lnlinks" href="#hl-13-37">37</a>
</span><span class="lnt" id="hl-13-38"><a class="lnlinks" href="#hl-13-38">38</a>
</span><span class="lnt" id="hl-13-39"><a class="lnlinks" href="#hl-13-39">39</a>
</span><span class="lnt" id="hl-13-40"><a class="lnlinks" href="#hl-13-40">40</a>
</span><span class="lnt" id="hl-13-41"><a class="lnlinks" href="#hl-13-41">41</a>
</span><span class="lnt" id="hl-13-42"><a class="lnlinks" href="#hl-13-42">42</a>
</span><span class="lnt" id="hl-13-43"><a class="lnlinks" href="#hl-13-43">43</a>
</span><span class="lnt" id="hl-13-44"><a class="lnlinks" href="#hl-13-44">44</a>
</span><span class="lnt" id="hl-13-45"><a class="lnlinks" href="#hl-13-45">45</a>
</span><span class="lnt" id="hl-13-46"><a class="lnlinks" href="#hl-13-46">46</a>
</span><span class="lnt" id="hl-13-47"><a class="lnlinks" href="#hl-13-47">47</a>
</span><span class="lnt" id="hl-13-48"><a class="lnlinks" href="#hl-13-48">48</a>
</span><span class="lnt" id="hl-13-49"><a class="lnlinks" href="#hl-13-49">49</a>
</span><span class="lnt" id="hl-13-50"><a class="lnlinks" href="#hl-13-50">50</a>
</span><span class="lnt" id="hl-13-51"><a class="lnlinks" href="#hl-13-51">51</a>
</span><span class="lnt" id="hl-13-52"><a class="lnlinks" href="#hl-13-52">52</a>
</span><span class="lnt" id="hl-13-53"><a class="lnlinks" href="#hl-13-53">53</a>
</span><span class="lnt" id="hl-13-54"><a class="lnlinks" href="#hl-13-54">54</a>
</span><span class="lnt" id="hl-13-55"><a class="lnlinks" href="#hl-13-55">55</a>
</span><span class="lnt" id="hl-13-56"><a class="lnlinks" href="#hl-13-56">56</a>
</span><span class="lnt" id="hl-13-57"><a class="lnlinks" href="#hl-13-57">57</a>
</span><span class="lnt" id="hl-13-58"><a class="lnlinks" href="#hl-13-58">58</a>
</span><span class="lnt" id="hl-13-59"><a class="lnlinks" href="#hl-13-59">59</a>
</span><span class="lnt" id="hl-13-60"><a class="lnlinks" href="#hl-13-60">60</a>
</span><span class="lnt" id="hl-13-61"><a class="lnlinks" href="#hl-13-61">61</a>
</span><span class="lnt" id="hl-13-62"><a class="lnlinks" href="#hl-13-62">62</a>
</span><span class="lnt" id="hl-13-63"><a class="lnlinks" href="#hl-13-63">63</a>
</span><span class="lnt" id="hl-13-64"><a class="lnlinks" href="#hl-13-64">64</a>
</span><span class="lnt" id="hl-13-65"><a class="lnlinks" href="#hl-13-65">65</a>
</span><span class="lnt" id="hl-13-66"><a class="lnlinks" href="#hl-13-66">66</a>
</span><span class="lnt" id="hl-13-67"><a class="lnlinks" href="#hl-13-67">67</a>
</span><span class="lnt" id="hl-13-68"><a class="lnlinks" href="#hl-13-68">68</a>
</span><span class="lnt" id="hl-13-69"><a class="lnlinks" href="#hl-13-69">69</a>
</span><span class="lnt" id="hl-13-70"><a class="lnlinks" href="#hl-13-70">70</a>
</span><span class="lnt" id="hl-13-71"><a class="lnlinks" href="#hl-13-71">71</a>
</span><span class="lnt" id="hl-13-72"><a class="lnlinks" href="#hl-13-72">72</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">gen_tokens</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">attention_scores</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">SamplerConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="n">clarifying_question_token</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2564</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1337</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">metrics</span> <span class="o">=</span> <span class="n">calculate_metrics</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">attention_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">**</span><span class="n">ent</span><span class="o">**</span><span class="p">,</span> <span class="o">**</span><span class="n">vent</span><span class="o">**</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;logits_entropy&#34;</span><span class="p">],</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;logits_varentropy&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_ent</span><span class="p">,</span> <span class="n">attn_vent</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_entropy&#34;</span><span class="p">],</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_varentropy&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">agreement</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;agreement&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">interaction_strength</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;interaction_strength&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Low Entropy, Low Varentropy: &#34;flowing with unspoken intent&#34; </span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">**</span><span class="n">ent</span><span class="o">**</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">low_ent_thresh</span> <span class="ow">and</span> <span class="o">**</span><span class="n">vent</span><span class="o">**</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">low_vent_thresh</span><span class="p">:</span> <span class="o">//</span><span class="n">注意这里是通过ent</span> <span class="n">和</span> <span class="n">vent去判断采用哪一种adaption策略</span><span class="err">。</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># High Entropy, Low Varentropy: &#34;treading carefully, asking clarifying questions&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">ent</span> <span class="o">&gt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">high_ent_thresh</span> <span class="ow">and</span> <span class="n">vent</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">low_vent_thresh</span><span class="p">:</span> <span class="o">//</span><span class="n">这里需要插入一个clarification问题</span><span class="err">，</span><span class="n">问清楚到底用户的问题</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Insert a clarifying question token if not already present</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">jnp</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">gen_tokens</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">clarifying_question_token</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">clarifying_question_token</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># If we&#39;ve just asked a question, sample with slightly higher temperature</span>
</span></span><span class="line"><span class="cl">            <span class="n">temp_adj</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">helv_attn_ent_offset</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">helv_attn_ent_coef</span> <span class="o">*</span> <span class="n">attn_ent</span>  <span class="c1"># Increase temperature based on attention entropy</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">temp</span> <span class="o">*</span> <span class="n">temp_adj</span><span class="p">),</span> <span class="n">top_p</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_p</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">min_p</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Low Entropy, High Varentropy: &#34;exploring forks in the path&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">ent</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">high_ent_thresh</span> <span class="ow">and</span> <span class="n">vent</span> <span class="o">&gt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">high_vent_thresh</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">temp_adj</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">lehv_interaction_strength_offset</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">lehv_interaction_strength_coef</span> <span class="o">*</span> <span class="n">interaction_strength</span>  <span class="c1"># Increase temperature based on interaction strength</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_k_adj</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_k</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">agreement</span><span class="p">))))</span>  <span class="c1"># Increase top_k when agreement is low</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">temp</span> <span class="o">*</span> <span class="n">temp_adj</span><span class="p">),</span> <span class="n">top_p</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k_adj</span><span class="p">,</span> <span class="n">min_p</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">min_p</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># High Entropy, High Varentropy: &#34;resampling in the mist&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">ent</span> <span class="o">&gt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">med_ent_thresh</span> <span class="ow">and</span> <span class="n">vent</span> <span class="o">&gt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">high_vent_thresh</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Use high temperature and adjusted top_p based on attention metrics</span>
</span></span><span class="line"><span class="cl">        <span class="n">temp_adj</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">hehv_attn_vent_offset</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">hehv_attn_vent_coef</span> <span class="o">*</span> <span class="n">attn_vent</span>  <span class="c1"># Increase temperature based on attention varentropy</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_p_adj</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">top_p</span> <span class="o">-</span> <span class="n">cfg</span><span class="o">.</span><span class="n">hehv_attn_ent_coef</span> <span class="o">*</span> <span class="n">attn_ent</span><span class="p">)</span>  <span class="c1"># Decrease top_p when attention entropy is high</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">temp</span> <span class="o">*</span> <span class="n">temp_adj</span><span class="p">),</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p_adj</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_p</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">min_p</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Middle ground: use adaptive sampling</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits_uncertainty</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;logits_entropy&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;logits_varentropy&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_uncertainty</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_entropy&#34;</span><span class="p">]</span> <span class="o">+</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_varentropy&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">temp</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_temp_logits</span> <span class="o">*</span> <span class="n">logits_uncertainty</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_temp_attn</span> <span class="o">*</span> <span class="n">attn_uncertainty</span> <span class="o">-</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_temp_agree</span> <span class="o">*</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;agreement&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_p</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_top_p</span> <span class="o">*</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_varentropy&#34;</span><span class="p">]),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">top_k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">top_k</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_top_k_int</span> <span class="o">*</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;interaction_strength&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_top_k_agree</span> <span class="o">*</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;agreement&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())),</span>
</span></span><span class="line"><span class="cl">            <span class="n">a_min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">a_max</span><span class="o">=</span><span class="mi">100</span>
</span></span><span class="line"><span class="cl">        <span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">min_p</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">min_p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_min_p</span> <span class="o">*</span> <span class="n">logits_uncertainty</span><span class="p">),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">n_adaptive_samples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">sample_key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">sample</span> <span class="o">=</span> <span class="n">_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">min_p</span><span class="o">=</span><span class="n">min_p</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">sample_key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">score_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="o">*</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence_score</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;logits_entropy&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_score_logits_ent</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_entropy&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_score_attn_ent</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;logits_varentropy&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_score_logits_vent</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;attn_varentropy&#34;</span><span class="p">])</span> <span class="o">*</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_score_attn_vent</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;agreement&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_score_agree</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="n">metrics</span><span class="p">[</span><span class="s2">&#34;interaction_strength&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">cfg</span><span class="o">.</span><span class="n">ada_score_int</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">log_prob</span> <span class="o">+</span> <span class="n">confidence_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">sample_scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">score_sample</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">best_sample_idx</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_scores</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">samples</span><span class="p">[</span><span class="n">best_sample_idx</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可能大家对clarification insertion比较难理解，这里举一个例子来说名：</p>
<p>例如</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt" id="hl-14-1"><a class="lnlinks" href="#hl-14-1">1</a>
</span><span class="lnt" id="hl-14-2"><a class="lnlinks" href="#hl-14-2">2</a>
</span><span class="lnt" id="hl-14-3"><a class="lnlinks" href="#hl-14-3">3</a>
</span><span class="lnt" id="hl-14-4"><a class="lnlinks" href="#hl-14-4">4</a>
</span><span class="lnt" id="hl-14-5"><a class="lnlinks" href="#hl-14-5">5</a>
</span><span class="lnt" id="hl-14-6"><a class="lnlinks" href="#hl-14-6">6</a>
</span><span class="lnt" id="hl-14-7"><a class="lnlinks" href="#hl-14-7">7</a>
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Input</span><span class="p">:</span> <span class="s2">&#34;The best programming language for&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">用户输入的这个问题具有很强的疑惑</span><span class="err">，</span><span class="n">但是不是说模型不理解</span><span class="err">。</span>
</span></span><span class="line"><span class="cl"><span class="n">例如可以是Java</span><span class="err">，</span><span class="n">python等</span><span class="err">，</span><span class="n">模型都可以回答的很好</span><span class="err">。</span><span class="n">这也是为什么这里的是</span><span class="o">**</span><span class="n">High</span> <span class="n">Entropy</span><span class="p">,</span> <span class="n">Low</span> <span class="n">Varentropy</span>
</span></span><span class="line"><span class="cl"><span class="n">所以</span><span class="err">，</span><span class="n">最好的解决策略是clarification</span> <span class="n">insertion</span><span class="err">。</span><span class="n">即插入一个问题</span><span class="err">，</span><span class="n">进一步说明问题到底是什么</span><span class="err">？</span><span class="o">**</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Output</span><span class="p">:</span> <span class="s2">&#34; [CLARIFY] What specific task or criteria are you considering?&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://LiuChaoXD.github.io/tags/llm/">LLM</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://LiuChaoXD.github.io/">ChaosThoughts.com</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>





<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
